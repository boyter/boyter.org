<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Processing 40 TB of code from ~10 million projects with a dedicated server and Go for $100</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<h1>Processing 40 TB of code from ~10 million projects with a dedicated server and Go for $100</h1>
					<aside class="notes"></aside>
				</section>

				<section>
					<h2>Intro?</h2>
					<p>
					<p>I blog <a href="https://boyter.org/">boyter.org</a> I free software <a href="https://github/boyter/">github/boyter/</a> I run <a href="https://searchcode.com/">searchcode.com</a> also on the twitter <a href="https://twitter.com/boyter">@boyter</a> activitypub <a href="https://honk.boyter.org/boyter">@boyter@honk.boyter.org</a></p>
					<svg style="height:100px;" class="sc-fqkvVR crilYZ kablamo-svg-logo sc-eldPxv bzdxPy" id="Layer_1" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 427.99 516.63" fill="#F6F6F6"><title>kablamo-logo</title><path d="M308,374.7l-3.19-8.45h-1.35v9.62h.92v-8.31h0l3.17,8.34h.86l3.29-8.29h0v8.32h.91V366.3h-1.34ZM295,367l3.34,0v8.84h.92V367l3.35,0v-.78l-7.61-.06Zm-85.95-122,38.55-4.51L169.22,358.91l60.13-.17L237.83,339l30.39,0-1.64,19.74,46.61,0,2.39-154.92H230.15Zm53.53,36.52,1.08-3,1.34-3c1.45-3.95.71-1.86,2.46-6.27,3.16-8.13,2.4-5.81,6.05-16,1.06-2.79,2.78-7,4.63-12.31l-.53,4.88L276,255.41c-.88,5.81-1.34,10-1.59,12.31l-1.21,11.15a71.44,71.44,0,0,0-.71,8.82l-1.71,18.12H252.07Zm94.6-263-60.2,0L270.13,129,263.92,18.88l-63.41,0L143.79,150.51l-12-46L194.05,18.3l-54.64,0L90.46,94.87l19.73-76.62-51.72,0L15.09,196.9l51.72,0,18.87-82.24L100.12,197l44.28,0,8.26-26.11,99.9-10.58L233.66,197l40.31,0h0l52.1,0c37.4,0,67.83-24.6,71.41-57.56,2.28-20.89-6.32-30.55-30.48-34,25.94-2.13,42.85-17.93,45.56-42.85C415.73,33.45,396.91,18.43,357.18,18.41ZM223.57,66.94l-1,11.22c-.53,6.68-.71,11.48-.81,14.16l-.48,12.82c-.18,5.07-.32,8.81-.08,10.15l-.48,20.83H199.08l10.1-28,1-3.47,1.3-3.48c1.33-4.54.66-2.13,2.31-7.21,3-9.35,2.29-6.68,5.65-18.43,1-3.2,2.63-8,4.32-14.15ZM345,138.43c-1.31,12.06-11.82,20.09-25.55,20.08a33.46,33.46,0,0,1-4.65-.26l8.37-34A24.07,24.07,0,0,1,327,124C339.57,124,346,129.33,345,138.43Zm14.41-68c-1.13,10.45-10.65,17.41-23.8,17.4h-3.8l7.41-30.54A45.21,45.21,0,0,1,344.8,57C355.32,57,360.37,61.55,359.41,70.39ZM238.16,363.1c-14.88,0-28.79,5.67-40.43,15.53l3.79-13.21-36.59,0-57.42,86.82,20.72-86.85-44.6,0-67.3,130.4,40.57,0,24.67-54.35c8.49-20.52,8.39-20.13,11.88-28.73-1.57,5.27-2.38,8.4-3.73,13.68L88,433,74.42,495.78h30.75l34.59-58.05c4.48-7.43,9-15.84,13.71-25-1.39,3.72-2.36,6.65-3,8.41-4,11.34-3.64,10-5.42,15.25l-20.42,59.44,39.51,0,5.44-19c6.33,13.67,18.89,21.56,36.16,21.57,35.25,0,66.27-35.8,71.38-82.8C280.59,383.5,265.5,363.12,238.16,363.1Zm-.43,53.08c-2.94,27-14.31,49.15-25.21,49.14-6.83,0-9.5-6.86-7.93-21.35,2.81-25.85,14.91-48.76,25.29-48C236.89,396.4,239.22,402.47,237.73,416.18ZM88.63,203.87,51.15,358.25l78.76.06,8.87-37.16-33.69,0,37.12-117.26Z"></path></svg>
					<p>
					Largest dataset: ~6PB<br>
Largest table: 2 trillion rows<br>
Highest QPS: 70,000/s under a DDoS<br>
					</p>
					<aside class="notes">
My name is Ben Boyter. You can find me online by the things you see up there. I am a tech principal at Kablamo specializing in 
data & applications. We are a digital product company and I have a passion for data platforms and search engines.

So this talk is as the title suggests about my experience Processing 40 TB of code from ~10 million projects with a dedicated 
server and Go for $100.

So im going to go through the reason for why I was doing this. The descriptions of what things I tried, the solution I settled on,
and where I plan on taking this in the future. I will mix in some of the more interesting findings I got though as well so be 
on the lookout for those.

I thought id throw in some stats oif the largest data sets I have worked on there.
6PB of data
a table with 2 trillion rows (it grew by about 200 million every day)
And the highest QPS I have dealt with which was ovber 70,000 rps while under a DDoS attack.

Oh and in full disclosure the title is slightly clickbait. I didn't actually analyze 10 million projects. I was short by
15,000 and rounded up. Please forgive me.
					</aside>
				</section>


				<section>
					<h2>What?</h2>
					<p>
					What did I try?<br>
					What did I learn?<br>
					What worked?<br>
					The future?<br>
					</p>
					<aside class="notes">
So I am trying a newish format for this talk, which is a common one where I outline what im going to go through. So here it is.
What did I try, how did I faile, what did I learn, what worked eventually, and future thoughts.
					</aside>
				</section>

				
				<section>
					<section>
						<h2>Why?</h2>
						<p>
						Why would anyone in their right mind do this?
						</p>
						<img src="./img/why.gif" style="border: none;" />
						<aside class="notes">
I get asked this a lot by my managers....
I don't have a good reason for why I do anything outside of work to be honest. I just do things... 
sometimes I get something useful out of and write about it, or record it. I almost always have fun doing it
so perhaps thats the reason.

But for this talk I do have some answers.

Why Go? Why dedicated server?

As for why I made any decision... its mostly down to personal learning, and iterating. 
I was never looking for an optimial way to do this, and what I got was organic. There are far more
efficient ways to achive what I did given some thinking ahead, so please refrain from yelling at me with them.
I am aware, I just didn't care too much at the time.

Thats not a why though...

The why is that I write a lot of code outside of work and there are two projects I maintain which are relevant to this talk.
						</aside>
					</section>
					<section>
						<h2>scc</h2>
						<pre><code style="font-size: 14px; line-height: 1em;">$ scc redis 
───────────────────────────────────────────────────────────────────────────────
Language                 Files     Lines   Blanks  Comments     Code Complexity
───────────────────────────────────────────────────────────────────────────────
C                          296    180267    20367     31679   128221      32548
C Header                   215     32362     3624      6968    21770       1636
TCL                        143     28959     3130      1784    24045       2340
Shell                       44      1658      222       326     1110        187
Autoconf                    22     10871     1038      1326     8507        953
Lua                         20       525       68        70      387         65
Markdown                    16      2595      683         0     1912          0
Makefile                    11      1363      262       125      976         59
Ruby                        10       795       78        78      639        116
gitignore                   10       162       16         0      146          0
YAML                         6       711       46         8      657          0
HTML                         5      9658     2928        12     6718          0
C++                          4       286       48        14      224         31
License                      4       100       20         0       80          0
Plain Text                   3       185       26         0      159          0
CMake                        2       214       43         3      168          4
CSS                          2       107       16         0       91          0
Python                       2       219       12         6      201         34
Systemd                      2        80        6         0       74          0
BASH                         1       118       14         5       99         31
Batch                        1        28        2         0       26          3
C++ Header                   1         9        1         3        5          0
Extensible Styleshe…         1        10        0         0       10          0
Smarty Template              1        44        1         0       43          5
m4                           1       562      116        53      393          0
───────────────────────────────────────────────────────────────────────────────
Total                      823    271888    32767     42460   196661      38012
───────────────────────────────────────────────────────────────────────────────
Estimated Cost to Develop (organic) $6,918,301
Estimated Schedule Effort (organic) 28.682292 months
Estimated People Required (organic) 21.428982
───────────────────────────────────────────────────────────────────────────────
Processed 9425137 bytes, 9.425 megabytes (SI)
───────────────────────────────────────────────────────────────────────────────
</code></pre>
						<aside class="notes">
The first is a tool called Sloc Cloc and Code (scc). Its a command line tool which given a code repository will 
count all of the lines of code, comments, blank lines in multiple programming languages.

It has one additional trick which is that it will estimate cyclomatic complexity by counting branch statements in the code. 
However this is a point of contention. Saying that a file, or project has a complexity of 200 is not very useful without some 
context. Is that high? Low? 

So I thought I would start collecting data from a few repositories in order to have an idea what this value actually 
means. I started getting the most popular repositories I could think of and was running scc over those, before realizing 
this was going to take a while and I was going to miss out on a lot of languages and repository sizes.

So I figured why not run it over every repository I could find?
						</aside>
					</section>

					<section>
						<img src="./img/searchcode.png" height="500px;" style="border: none;" />
						<aside class="notes">
This is where another project I maintain comes in. I run the site searchcode.com which as the name suggests searches code. 
It also at the time was aware of almost 10 million repositories across gitlab, bitbucket and github.

So the idea was to export all 10 million repositories and then run scc over those and do something with the results.
						</aside>
					</section>
				</section>

				<section>
					<h2>Attempt 1</h2>
					<img src="./img/nbn.jpg" style="border: none;" />
					<aside class="notes">
So for the first attempt.

I tried running it locally on my desktop. A very old desktop... but good enough for something like this. 

From memory I wrote a very simple shell script to process each repository and save the results to disk.

At the time I didn't have NBN, however they had installed the node, which reduced my ADSL2+ speed from a respectable 21 mbps to 3 mbps.

So when I was running my process it used all of the bandwidth I had available.
This resulted in some complaints from the children and then wife about Netflix and Youtube having issues playing. 

The first rule of marriage is don't annoy your partner. So I started looking at other ways to do this that did not invovle tying up
my own internet connection for weeks.
				</section>

				<section>
					<h2><img src="./img/pin.png" style="border: none; background:none; height:0.6em;" /> Most common filenames?</h2>
					<p>
					</p>
					<table>
					<tr><td>makefile</td><td>59,141,098</td></tr>
					<tr><td>index</td><td>33,962,093</td></tr>
					<tr><td>readme</td><td>22,964,539</td></tr>
					<tr><td>jquery</td><td>20,015,171</td></tr>
					<tr><td>main</td><td>12,308,009</td></tr>
					<tr><td>package</td><td>10,975,828</td></tr>
					<tr><td>license</td><td>10,441,647</td></tr>
					<tr><td>_init_</td><td>10,193,245</td></tr>
					</table>
					<aside class="notes">
Ah so the first interesting finding.

Probably the first thing I started looking at, and sorry for you one of the most boring.

Had you asked me before I started this I would have said, README, main, index, license. Thankfully the results reflect my thoughts pretty well. 
Although there are a lot of interesting ones in there. I have no idea why so many projects contain a file called 15 or s15 which I found in the full list.

JavaScript projects including Font Awesome, which stores the bathtub icon as s15 for some reason. 15 seems to be related to javascript as well.

It also looks based on the values that reports of jquery's death are greatly exaggerated.
					</aside>
				</section>

				<section>
					<h2>Attempt 2</h2>
					<img src="./img/attempt2.png" style="border: none;" /> <br>
					
					<img src="https://camo.githubusercontent.com/d6f56f2545f03c6d9c4fc822ea57df94436b389453d79e32d90ee9dc98df6cb4/68747470733a2f2f736c6f632e78797a2f6769746875622f626f797465722f7363632f" style="border: none;" />
					<img src="https://camo.githubusercontent.com/6202501b41ad1c3e212e21d07114e6ab13e54e5ec19adc0707311454b2dd517c/68747470733a2f2f736c6f632e78797a2f6769746875622f626f797465722f7363632f3f63617465676f72793d636f6465" style="border: none;" />
					<img src="https://camo.githubusercontent.com/d6e001cd08d58c4dbb5f66496ff564318abb28fbc5317c3fb1206a242d0628e1/68747470733a2f2f736c6f632e78797a2f6769746875622f626f797465722f7363632f3f63617465676f72793d626c616e6b73" style="border: none;" />
					<img src="https://camo.githubusercontent.com/978695417d8d0e34bc48c644eb18ce0fe90daaf164a02ee28f8ebbd121f0f1ba/68747470733a2f2f736c6f632e78797a2f6769746875622f626f797465722f7363632f3f63617465676f72793d636f6d6d656e7473" style="border: none;" />
					<img src="https://camo.githubusercontent.com/1a6c6060cac1fe672b424b6cf4e2bc549caf154e6ddbf595f3c597c9ee184497/68747470733a2f2f736c6f632e78797a2f6769746875622f626f797465722f7363632f3f63617465676f72793d636f636f6d6f" style="border: none;" />
					
					<aside class="notes">
So for Kablamo I work with AWS a lot. So naturally the first thought was to use AWS Lambda. 

After all we know serverless has a lot going for it and should be cheap.

I had previously set this up to produce those github badges you see allowing you to display things 
like the lines of code on your github project and you can see an example at the bottom here.

This worked as follows. I setup a aws lambda function behind a ALB. Calls to it spawn a few subprocesses. The first being a shallow git clone into the tmp location of lambda. https://github.com/boyter/scc-lambda then it runs scc over the checked out code, and uploads the results into s3.

Perfect solution you might think.

Lambdas have a lot going for them. They are reasonably clean from a developer perspective because you don't have to manage any servers. 
They scale to thousands and thousands of concurrent requests and they generally don't cost very much.

However there are of course some limitations, but I was prepared to live with. The first is that http lambdas like this have a hard 29 second timeout. Which chould be a problem for larger repositories.

So I started by exporting all of the git urls, and wrote a simple python script to loop though every repository 
and ping my URL to generate the the data.

After processing 1 million repositories I checked my AWS costs and noticed they were about $60.

This meant to do all 10 million repositories I was looking at a $700 AWS bill. Ouch. 

As such I decided to rethink my solution. Keep in mind that was mostly storage and CPU, or what was needed to collect 
this information. Assuming I processed or exported the data it was going to increase the cost considerably. 
				</section>

				<section>
					<h2><img src="./img/pin.png" style="border: none; background:none; height:0.6em;" />How many “pure” projects</h2>
					<p>
					<img src="./img/languagesPerProject.png" style="border: none;" />
					</p>
					<aside class="notes">
Another finding here.

So I defined a pure project to mean a project that has 1 language in it. 
Of course that would not be very interesting by itself, so lets see what the spread is. 
As it turns out most projects have fewer than 25 languages in them with most in the less than 10 bracket.

The peak in the graph is for 4 languages.

Of course pure projects might only have one programming language, but have lots of supporting other formats such as 
markdown, json, yml, css, .gitignore which are picked up by scc. It's probably reasonable to assume that any project with 
less than 5 languages is “pure”  and as it turns out is just over half the total data set. Of course your definition of purity might be different to mine so feel free to adjust to whatever number you like.

What suprises me is an odd bump around 34-35 languages. 
I have no reasonable explanation as to why this might be the case and it probably warrents some investigation.
					</aside>
				</section>

				<section>
					<h2>Attempt 3</h2>
					<img src="./img/attempt3.png" style="border: none;" /> <br>
					<aside class="notes">		
So attempt 3.

Since I was already in AWS the hip solution would be to dump the git repository locations as messages 
into SQS and pull from it using EC2 instances or fargate for processing. Then scale out like crazy. 
I actually mentioned this to people at work and thats the solution they also proposed.

Cool, scale out to collect the data. I can do that.

Collecting the data was one thing, but what about querying it?

The idea was to use AWS Athena after putting the data into S3. We can then query the data directly in S3 which is cool! 
Plus its serverless and in theory should be cheap.

Thankfully at the time I was working on a customer project using Athena, and I was able to use that to estimate what 
it would have cost at the time. So I ran what I expected the data size to be and it was going to cost 
something like $2.50 USD. Per query! 

Nobody gets their queries right at first and that's half the price of a coffee for a single For that dataset I quickly 
looked for an alternative.

Also thinking about it, setting up the SQS queue, running instances... its going to be either click-ops 
development, or a pain to get everything setup properly with devops stacks and I was feeling lazy...
					</aside>
				</section>

				<section>
					<h2><img src="./img/pin.png" style="border: none; background:none; height:0.6em;" />YAML or YML?</h2>
					<table>
					<tr><td>yaml</td><td>3,572,609</td></tr>
					<tr><td>yml</td><td>14,076,349</td></tr>
					</table>
					<aside class="notes">
So on the subject of devops and yaml....

Sometime back on the company slack there was a “discussion” with many dying on one hill or the other over the use 
of .yaml or .yml for stack definitions.

As such I thought I would calculate which one is more comment so. The debate can finally be ended. Although I suspect some 
will still prefer to die on their chosen hill.

Perhaps its just an easy thing to bike shed about.
					</aside>
				</section>

				<section>
					<section>
						<h2>Attempt 4</h2>
						<img src="./img/tacobell.png" height="500px;" style="border: none;" /> <br>
						<aside class="notes">
To paraphrase the infamous mongodb is web-scale video, as architects and developers, there is a tendency for us to read sites like High Scalability and start thinking we are google architects with google scale problems.

There is a great post out there on this subject with the title "Taco Bell programming" Its core thesis is that you can solve most problems with the unix tool set, and a bit some judicious use of the pipe operator and redirects. The author gives a wonderful example about a 32 process crawler to download URL's.

Now I don't subscribe entirely to that idea of doing everything in the shell, probably because I am not at all fluent at sed, but I do think that we need to stop thinking of millions to billions of rows, terabytes of data is a large scale problem. The original map-reduce paper by google came out in 2004 and hadoop came out in 2007. That's over 16 years ago. A lot of the showcases for what those projects could do can be easily done by a single machine these days.

At the time I had some spare compute used by searchcode. It was fronted by a varnish cache accelerator, and if you have ever used varnish you know its very CPU efficient. As such the CPU on that box was doing the square root of zero most of the time, so I had a look at running the process on it.

What I did was write a simple Go program that spawned 32 process  which literally called out using spawned processes to shallow git clone to a temp directory then run scc over it, and save the resulting json file to disk and s3.
						</aside>
					</section>
					<section>
						<h2>Attempt 4</h2>
						<img src="./img/hetzner.png" style="border: none;" /> <br>
						<img src="./img/hetzner2.png" height="500px;" style="border: none;" />
						<aside class="notes">
						

						</aside>
					</section>
					<section>
						<h2>Attempt 4</h2>
						<img src="./img/running.png" style="border: none;" /> <br>
						<aside class="notes">
						

						</aside>
					</section>

					<section>
						<h2>Why Go?</h2>
						<img src="./img/go.jpg" style="border: none;" /> <br>
						<aside class="notes">
I initally wrote the script in Python but having to install the pip dependencies on my clean 
varnish box seemed like a bad idea and it keep breaking in odd ways which I didnt feel like debugging.

I have a rule these days with Python. 
If I have to import something not in the standard library, 
I am going to run it more than 5 times
or its a task that takes longer than an a few hours to run I rewrite it unless it worked correctly the first time.
						</aside>
					</section>
					<section>
						<h2>Channels and Pipes </h2>
						<pre><code style="font-size: 14px; line-height: 1em;">
cat urllist.txt | xargs -P16 python parse.py
						</code></pre>

						<pre><code style="font-size: 14px; line-height: 1em;">
ch := make(chan string)

for i:=0;i<16;i++{
	go parse(ch)
}

for _, l := range urllist {
	ch <- l
}
						</code></pre>
						<aside class="notes">
Going back to taco bell programming, Go fits in with this rather well. 
It produces a single binary to deploy and run.
Its reasonably fast.
Its use of channels reminds me of pipes in the unix sense.
Its opinionated with generally one way to do anything well.

You an see an example of unix pipes at the top where we spawn 16 processes to parse from a list.
The Go code below it is similar in that we spawn 16 goroutines which approximates the same thing.

The advantage of the Go code of course is that its not limited to lines, I can change from the string
to a real type to add addtional bits of information if needed.
						</aside>
					</section>
				</section>

				<section>
					<section>
						<h2><img src="./img/pin.png" style="border: none; background:none; height:0.6em;" />Files in a repo?</h2>
						<p>
						<img src="./img/filesPerProject.png" style="border: none;" />
						</p>
						<aside class="notes">
How many files are in an average repository? Do most projects have a few files in them, or many? 
By looping over the repositories and counting the number of files we can then drop them in buckets of 1, 2, 10, 12 
or however many files it has and plot it out.					

This is limited to projects with less than 1000 files because the plot looks like empty with a thin smear on the 
left side if you include all the outliers.
</aside>
					</section>
					<section>
						<h2><img src="./img/pin.png" style="border: none; background:none; height:0.6em;" />Files in a repo 95%</h2>
						<p>
						<img src="./img/filesPerProjectPercentile95.png" style="border: none;" />
						</p>
						<aside class="notes">
As it turns out most repositories have less than 200 files in them.

However what about plotting this by percentile, or more specifically by 95th percentile so its actually worth looking at? 
Turns out the vast majority 95% of projects have less than 1,000 files in them. While 90% of them have less than 
300 files and 85% have less than 200.
						</aside>
					</section>
				</section>

				<section>
					<section>
						<h2>Processing...</h2>
						<p>
						</p>
						<aside class="notes">
The size of the data I needed to process raised another question. 
How does one process 10 million JSON files taking up just over 1 TB of disk space in an S3 bucket?

Keeping in mind we already ruled out Athena. 

Firstly why it it in JSON? Thats just because thats the output from scc and I didn't feel like reprocessing it. Remember this is an iterative project.

The brains trust at work raised the idea to dump the data into a large SQL database. H
owever this means processing the data into the database, then running queries over it multiple times.
 Plus the structure of the data meant having a few tables which means foreign keys and indexes to ensure some level of performance. 

This feels wasteful because we could just process the data as we read it from disk in a single pass, assuming 
we know what we want up front. 

I was also worried about building a database this large. With just data it would be over 1 TB in size 
before adding indexes. Now normally I would not worry about this, but I have run into issues with 
large databases of this size where I spend a fair amount of time turning them in order to get prompt responses.

Probably not a problem if the HDD was a SSD, but for my box it was a mechanical drive.

So I pulled the data out of S3 once and stored each file into a tar file that I could process over and over. 
This was something I regret having to do because if I have just kept a local version I could probably have 
processed faster and saved some cost.
						</aside>
					</section>
					<section>
						<h2>Go again...</h2>
						<p>
<pre><code style="font-size: 14px; line-height: 1em;">
filesPerProject := map[int64]int64{}      // Number of files in each project in buckets IE projects with 10 files or projects with 2
projectsPerLanguage := map[string]int64{} // Number of projects which use a language
filesPerLanguage := map[string]int64{}    // Number of files per language
hasLicenceCount := map[string]int64{}     // Count of if a project has a licence file or not

fileNamesCount := map[string]int64{}                     // Count of filenames
fileNamesNoExtensionCount := map[string]int64{}          // Count of filenames without extensions
fileNamesNoExtensionLowercaseCount := map[string]int64{} // Count of filenames tolower and no extensions
complexityPerLanguage := map[string]int64{}              // Sum of complexity per language

commentsPerLanguage := map[string]int64{} // Sum of comments per language

sourceCount := map[string]int64{} // Count of each source github/bitbucket/gitlab

ymlOrYaml := map[string]int64{} // yaml or yml extension?

mostComplex := Largest{}                               // Holds details of the most complex file
mostComplexPerLanguage := map[string]Largest{}         // Most complex of each file type
mostComplexWeighted := Largest{}                       // Holds details of the most complex file weighted by lines NB useless because it only picks up minified files
mostComplexWeightedPerLanguage := map[string]Largest{} // Most complex of each file type weighted by lines
largest := Largest{}                                   // Holds details of the largest file in bytes
largestPerLanguage := map[string]Largest{}             // largest file per language
longest := Largest{}                                   // Holds details of the longest file in lines
longestPerLanguage := map[string]Largest{}             // longest file per language
mostCommented := Largest{}                             // Holds details of the most commented file in lines
mostCommentedPerLanguage := map[string]Largest{}       // most commented file per language
</code></pre>
						</p>
						<aside class="notes">
Since I was already working in Go I decided to just process the data in Go.

Most as a learning exercise than anything else. 

I didn't multithread this for a few reasons. The first was that it was disk bound to the reading
of content off the disk, and the second was I didn't feel like making it thread safe.

It turned out to be a simple program. Create some maps and arrays, then loop every json file incrementing
the counts as needed based on some logic for whatever we are caluclating.

Now this is not ideal from an analytics perspective, it had a few advantages. The first was that I didn't need to parse the data into 
some format I could then query over. Which was fine so long as I knew what answeres I wanted up front. The other was that it allowed me to do things
that perhaps would not be easy or possible inside your usual formats.
						</aside>
					</section>
				</section>

				<section>
					<h2><img src="./img/pin.png" style="border: none; background:none; height:0.6em;" />The Java FactoryFactory</h2>
					<table>
					<tr><td>not factory</td><td>271,375,574</td><td>97.9%</td></tr>
					<tr><td>factory</td><td>5,695,568</td><td>2.09%</td></tr>
					<tr><td>factoryfactory</td><td>25,316</td><td>0.009%</td></tr>
					<tr><td>factoryfactoryfactory</td><td>0 :(</td><td></td></tr>
					</table>
					<aside class="notes">
Another one that came up in the internal company slack when looking through some old Java code. I thought why not add a check for any Java code that has Factory, FactoryFactory or FactoryFactoryFactory in the name. The idea being to see how many factories are out there.

So slightly over 2% of all the Java code that I checked appeared to be a factory or factoryfactory. Thankfully there are no factoryfactoryfactories and perhaps that joke can finally die, although I am sure at least one non-ironic one exist somewhere in some Java 5 monolith that makes more money every day than I will see over my entire working life.
					</aside>
				</section>

				<section>
					<h2>Raw Numbers</h2>
					<p>
	9,985,051 total repositories<br>
	9,100,083 repositories with at least 1 identified file<br>
	884,968 empty repositories (those with no files)<br>
	3,529,516,251 files in all repositories<br>
	40,736,530,379,778 bytes processed (40 TB)<br>
	1,086,723,618,560 lines identified<br>
	816,822,273,469 code lines identified<br>
	124,382,152,510 blank lines identified<br>
	145,519,192,581 comment lines identified<br>
	71,884,867,919 complexity count according to scc rules<br>
					</p>
					<aside class="notes">
It took about 5 weeks to download and run scc over the collection of repositories saving all of the data. 
3,024,000 seconds, so we processed about 3 repositories per second in that time.
This produced just over 1 TB of JSON files containing the results.
It took just over 49 hours to crunch the 1 TB of JSON and produce the results.
					</aside>
				</section>

				<section>
					<section>
						<h2>Lessons Learnt</h2>
						<p>
						Don't store lots of files in tmp<br>
						Don't use s3 at first...<br>
						Consider compression, suzh as zstd.<br>
						Keep results locally!<br>
						When CPU is high for a long time consider dedicated<br>
						Dedicated server go brrr<br>
						</p>
						<aside class="notes">
	Lessons learnt from this

	be careful when creation millions of files in /tmp/ if you ever reboot... because you end up locking your linux box for a long time while it clears those files. 

	Ask me how searchcode went down for several hours... I actually ended up having to boot into a recovery mode and then find the fastest way to clear all the files to bring it back online after waiting a few hours for the reboot process to do this

	Don't bother with the s3 saving at first, stuff the json into a tar file to some limit, either size or number of files, then compress and shove it into s3

	Consider other compression software... gzip is ubiquitous but zstd is probably the best other option offering better compression and faster performance, and for one off things like this really worth it

	If you are going to process outside of s3, be careful storing it there, because the cost to fetch it is really non trivial

	For tasks where you throw away the source data such as this, keep the results around locally! 
	Even if you plan on storing them someone having a local copy can really help

	You don't have to use the "pro" tools if you don't want, especailly if its for something that isnt designed to be repeatable.
						</aside>
					</section>

					<section>
						<h2>Dedicated server go brrr</h2>
						<img src="./img/brrr.png" style="border: none;" />
						<aside class="notes">
						</aside>
					</section>
				</section>


		
				<section>
					<h2><img src="./img/pin.png" style="border: none; background:none; height:0.6em;" />Missing a license</h2>
					<img src="./img/hasLicense.png" style="border: none;" />
					<aside class="notes">
This is an interesting one. This was only looking for files with the following names “license”, “licence”, “copying”, “copying3”, “unlicense”, “unlicence”, “license-mit”, “licence-mit” or “copyright”.

So based on that only about 1/3 of repositories have a licence.

This not exhaustive, because the files could have a license via some other means such as SPDX tags, or in the README file. However the results are still problematic.

I have heard some people say they say without a licence is now public domain. I have two issues with this.

The first reason is that its incorrect. You still need to explicitly say its public domain.

The second is that for anyone with Australian or German citizenship (im guessing most of you here), you legally cannot give up your human rights, with copyright being considered a human right. 

So you also cannot put anything into public domain. Germany implemented this right after WW2 so yes its done for a good reason.

So if you want your work to be as open as possible you must be explicit with your license. Consider MIT or Unlicence to have a similar affect. 
					</aside>
				</section>

								

				<section>
					<h2>The Future?</h2>
					<p>
					Keeping the URL properly...<br>
					Forget S3<br>
					Trie to store filenames (avoid lossy)<br>
					Use SQLite?<br>
					Latest version of scc<br>
					Determine maintainability**<br>
					Tabs vs Spaces<br>
					Don't store as JSON<br>
					Try bigquery?<br>
					Another language? Rust? Zig?<br>
					Explorer!<br>
					</p>
					<aside class="notes">
I am planning on doing this again at some point with the latest version of scc and an updated list of projects from searchcode.

I think the first thing to do is forget s3 and look at packing data into tar files with gz up to some limit.
Id like to try sqlite as the store as well, since that would make for a great blog post title.
Id love an algorithm to determine maintainability. Reading a lot of academic papers on this but yet to find something useful.
Bigquery is one of those things that should eat something like this...

Id like to actually process the results too and provide a webpage you could use to compare your project to millions of others.
Maybe as a way to see what projects are like yours!
					</aside>
				</section>
		
				<section>
					<h2>Thank you!</h2>
					<p>
					<a href="https://boyter.org/posts/an-informal-survey-of-10-million-github-bitbucket-gitlab-projects/">https://boyter.org/posts/an-informal-survey-of-10-million-github-bitbucket-gitlab-projects/</a><br>
					<a href="https://boyter.org/">https://boyter.org/</a><br>
					<a href="https://github.com/boyter/scc-data">https://github.com/boyter/scc-data</a><br>
					<a href="https://github.com/boyter/scc">https://github.com/boyter/scc</a><br>
					<a href="https://news.ycombinator.com/item?id=21121735">https://news.ycombinator.com/item?id=21121735</a><br>
					<a href="https://boyter.org/static/dataenbytes2023/">https://boyter.org/static/dataenbytes2023/</a><br>
					<aside class="notes">
So thank you very much. I know this is a multitrack conference and if I were you I would have been in that LLM talk in 
one of the other rooms, but I really appreicate you having come and listened to my rambling.

If you have any questions please as me here, or if shy over email or twitter or whatever you prefer. 
I promise to respond and not ghost you.
					</aside>
				</section>

			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
