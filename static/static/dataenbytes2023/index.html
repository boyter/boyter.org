<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Processing 40 TB of code from ~10 million projects with a dedicated server and Go for $100</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<h1>Processing 40 TB of code from ~10 million projects with a dedicated server and Go for $100</h1>
					<aside class="notes"></aside>
				</section>

				<section>
					<h2>Intro?</h2>
					<p>
					<p>I blog <a href="https://boyter.org/">boyter.org</a> I free software <a href="https://github/boyter/">github/boyter/</a> I run <a href="https://searchcode.com/">searchcode.com</a> also on the twitter <a href="https://twitter.com/boyter">@boyter</a> activitypub <a href="https://honk.boyter.org/boyter">@boyter@honk.boyter.org</a></p>
					<svg style="height:100px;" class="sc-fqkvVR crilYZ kablamo-svg-logo sc-eldPxv bzdxPy" id="Layer_1" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 427.99 516.63" fill="#F6F6F6"><title>kablamo-logo</title><path d="M308,374.7l-3.19-8.45h-1.35v9.62h.92v-8.31h0l3.17,8.34h.86l3.29-8.29h0v8.32h.91V366.3h-1.34ZM295,367l3.34,0v8.84h.92V367l3.35,0v-.78l-7.61-.06Zm-85.95-122,38.55-4.51L169.22,358.91l60.13-.17L237.83,339l30.39,0-1.64,19.74,46.61,0,2.39-154.92H230.15Zm53.53,36.52,1.08-3,1.34-3c1.45-3.95.71-1.86,2.46-6.27,3.16-8.13,2.4-5.81,6.05-16,1.06-2.79,2.78-7,4.63-12.31l-.53,4.88L276,255.41c-.88,5.81-1.34,10-1.59,12.31l-1.21,11.15a71.44,71.44,0,0,0-.71,8.82l-1.71,18.12H252.07Zm94.6-263-60.2,0L270.13,129,263.92,18.88l-63.41,0L143.79,150.51l-12-46L194.05,18.3l-54.64,0L90.46,94.87l19.73-76.62-51.72,0L15.09,196.9l51.72,0,18.87-82.24L100.12,197l44.28,0,8.26-26.11,99.9-10.58L233.66,197l40.31,0h0l52.1,0c37.4,0,67.83-24.6,71.41-57.56,2.28-20.89-6.32-30.55-30.48-34,25.94-2.13,42.85-17.93,45.56-42.85C415.73,33.45,396.91,18.43,357.18,18.41ZM223.57,66.94l-1,11.22c-.53,6.68-.71,11.48-.81,14.16l-.48,12.82c-.18,5.07-.32,8.81-.08,10.15l-.48,20.83H199.08l10.1-28,1-3.47,1.3-3.48c1.33-4.54.66-2.13,2.31-7.21,3-9.35,2.29-6.68,5.65-18.43,1-3.2,2.63-8,4.32-14.15ZM345,138.43c-1.31,12.06-11.82,20.09-25.55,20.08a33.46,33.46,0,0,1-4.65-.26l8.37-34A24.07,24.07,0,0,1,327,124C339.57,124,346,129.33,345,138.43Zm14.41-68c-1.13,10.45-10.65,17.41-23.8,17.4h-3.8l7.41-30.54A45.21,45.21,0,0,1,344.8,57C355.32,57,360.37,61.55,359.41,70.39ZM238.16,363.1c-14.88,0-28.79,5.67-40.43,15.53l3.79-13.21-36.59,0-57.42,86.82,20.72-86.85-44.6,0-67.3,130.4,40.57,0,24.67-54.35c8.49-20.52,8.39-20.13,11.88-28.73-1.57,5.27-2.38,8.4-3.73,13.68L88,433,74.42,495.78h30.75l34.59-58.05c4.48-7.43,9-15.84,13.71-25-1.39,3.72-2.36,6.65-3,8.41-4,11.34-3.64,10-5.42,15.25l-20.42,59.44,39.51,0,5.44-19c6.33,13.67,18.89,21.56,36.16,21.57,35.25,0,66.27-35.8,71.38-82.8C280.59,383.5,265.5,363.12,238.16,363.1Zm-.43,53.08c-2.94,27-14.31,49.15-25.21,49.14-6.83,0-9.5-6.86-7.93-21.35,2.81-25.85,14.91-48.76,25.29-48C236.89,396.4,239.22,402.47,237.73,416.18ZM88.63,203.87,51.15,358.25l78.76.06,8.87-37.16-33.69,0,37.12-117.26Z"></path></svg>
					<p>
					Largest dataset: ~6PB<br>
Largest table: 2 trillion rows<br>
Highest QPS: 70,000/s under a DDoS<br>
					</p>
					<aside class="notes">My name is Ben Boyter. You can find me online by the things you see up there. I am a tech principal at Kablamo specializing in data & applications. We are a digital product company and I have a passion for data platforms and search engines.

So this talk is as the title suggests about my experience Processing 40 TB of code from ~10 million projects with a dedicated server and Go for $100.

So im going to go through the reason for why I was doing this. The descriptions of what things I tried, the solution I settled on, and where I plan on taking this in the future. I will mix in some of the more interesting findings I got though as well so be on the lookout for those.

Oh and in full disclosure the title is slightly clickbait. I didn't actually analyze 10 million projects. I was short by 15,000 and rounded up. Please forgive me.
					</aside>
				</section>


				<section>
					<h2>What?</h2>
					<p>
					What did I try?<br>
					What did I learn?<br>
					What did I work with.<br>
					The future?<br>
					</p>
					<aside class="notes">So this talk is as the title suggests about my experience Processing 40 TB of code from ~10 million projects with a dedicated server and Go for $100.

So im going to go through the reason for why I was doing this. The descriptions of what things I tried, the solution I settled on, and where I plan on taking this in the future. I will mix in some of the more interesting findings I got though as well so be on the lookout for those.

Oh and in full disclosure the title is slightly clickbait. I didn't actually analyze 10 million projects. I was short by 15,000 and rounded up. Please forgive me.
					</aside>
				</section>

				
				<section>
					<section>
						<h2>Why?</h2>
						<p>
						Why would anyone in their right mind do this?
						</p>
						<img src="./img/why.gif" style="border: none;" />
						<aside class="notes">I write a lot of code outside of work and there are two projects I maintain which are relevant to this talk. 
						</aside>
					</section>
					<section>
						<h2>scc</h2>
						<pre><code style="font-size: 14px; line-height: 1em;">$ scc redis 
───────────────────────────────────────────────────────────────────────────────
Language                 Files     Lines   Blanks  Comments     Code Complexity
───────────────────────────────────────────────────────────────────────────────
C                          296    180267    20367     31679   128221      32548
C Header                   215     32362     3624      6968    21770       1636
TCL                        143     28959     3130      1784    24045       2340
Shell                       44      1658      222       326     1110        187
Autoconf                    22     10871     1038      1326     8507        953
Lua                         20       525       68        70      387         65
Markdown                    16      2595      683         0     1912          0
Makefile                    11      1363      262       125      976         59
Ruby                        10       795       78        78      639        116
gitignore                   10       162       16         0      146          0
YAML                         6       711       46         8      657          0
HTML                         5      9658     2928        12     6718          0
C++                          4       286       48        14      224         31
License                      4       100       20         0       80          0
Plain Text                   3       185       26         0      159          0
CMake                        2       214       43         3      168          4
CSS                          2       107       16         0       91          0
Python                       2       219       12         6      201         34
Systemd                      2        80        6         0       74          0
BASH                         1       118       14         5       99         31
Batch                        1        28        2         0       26          3
C++ Header                   1         9        1         3        5          0
Extensible Styleshe…         1        10        0         0       10          0
Smarty Template              1        44        1         0       43          5
m4                           1       562      116        53      393          0
───────────────────────────────────────────────────────────────────────────────
Total                      823    271888    32767     42460   196661      38012
───────────────────────────────────────────────────────────────────────────────
Estimated Cost to Develop (organic) $6,918,301
Estimated Schedule Effort (organic) 28.682292 months
Estimated People Required (organic) 21.428982
───────────────────────────────────────────────────────────────────────────────
Processed 9425137 bytes, 9.425 megabytes (SI)
───────────────────────────────────────────────────────────────────────────────
</code></pre>
						<aside class="notes">The first is a tool called Sloc Cloc and Code (scc). Its a command line tool which given a code repository will count all of the lines of code, comments, blank lines in multiple programming languages.

						It has one additional trick which is that it will estimate cyclomatic complexity by counting branch statements in the code. However this is a point of contention. Saying that a file, or project has a complexity of 200 is not very useful without some context. Is that high? Low? 

	So I thought I would start collecting data from a few repositories in order to have an idea what this value actually means. I started getting the most popular repositories I could think of and was running scc over those, before realizing this was going to take a while and I was going to miss out on a lot of languages and repository sizes.

	So I figured why not run it over every repository I could find?
						</aside>
					</section>

					<section>
						<img src="./img/searchcode.png" height="500px;" style="border: none;" />
						<aside class="notes">This is where another project I maintain comes in. I run the site searchcode.com which as the name suggests searches code. It also at the time was aware of almost 10 million repositories across gitlab, bitbucket and github.

	So the idea was to run scc over all 10 million repositories and do something with the results.
						</aside>
					</section>
				</section>

				<section>
					<h2>Attempt 1</h2>
					<img src="./img/nbn.jpg" style="border: none;" />
					<aside class="notes">
I tried running it locally on my desktop. From memory I wrote a very simple shell script using the repositories I was aware of.

At the time I didn't have NBN, however they had installed the node, which reduced my speed from a respectable 20 mbps to 3 mbps.

This resulted in some complaints from the wife about Netflix and Youtube having issues. 

The first rule of marriage as a man is don't annoy your partner. So I started looking at other ways to do this.
				</section>

				<section>
					<h2><img src="./img/pin.png" style="border: none; background:none; height:0.6em;" /> Most common filenames?</h2>
					<p>
					</p>
					<table>
					<tr><td>makefile</td><td>59,141,098</td></tr>
					<tr><td>index</td><td>33,962,093</td></tr>
					<tr><td>readme</td><td>22,964,539</td></tr>
					<tr><td>jquery</td><td>20,015,171</td></tr>
					<tr><td>main</td><td>12,308,009</td></tr>
					<tr><td>package</td><td>10,975,828</td></tr>
					<tr><td>license</td><td>10,441,647</td></tr>
					<tr><td>_init_</td><td>10,193,245</td></tr>
					</table>
					<aside class="notes">
Probably the first thing I started looking at, and sorry for you one of the most boring.

Had you asked me before I started this I would have said, README, main, index, license. Thankfully the results reflect my thoughts pretty well. Although there are a lot of interesting ones in there. I have no idea why so many projects contain a file called 15 or s15 which I found in the full list.

It also looks like reports of jquery's death are greatly exaggerated.
					</aside>
				</section>

				<section>
					<h2>Attempt 2</h2>
					<img src="./img/attempt2.png" style="border: none;" /> <br>
					
					<img src="https://camo.githubusercontent.com/d6f56f2545f03c6d9c4fc822ea57df94436b389453d79e32d90ee9dc98df6cb4/68747470733a2f2f736c6f632e78797a2f6769746875622f626f797465722f7363632f" style="border: none;" />
					<img src="https://camo.githubusercontent.com/6202501b41ad1c3e212e21d07114e6ab13e54e5ec19adc0707311454b2dd517c/68747470733a2f2f736c6f632e78797a2f6769746875622f626f797465722f7363632f3f63617465676f72793d636f6465" style="border: none;" />
					<img src="https://camo.githubusercontent.com/d6e001cd08d58c4dbb5f66496ff564318abb28fbc5317c3fb1206a242d0628e1/68747470733a2f2f736c6f632e78797a2f6769746875622f626f797465722f7363632f3f63617465676f72793d626c616e6b73" style="border: none;" />
					<img src="https://camo.githubusercontent.com/978695417d8d0e34bc48c644eb18ce0fe90daaf164a02ee28f8ebbd121f0f1ba/68747470733a2f2f736c6f632e78797a2f6769746875622f626f797465722f7363632f3f63617465676f72793d636f6d6d656e7473" style="border: none;" />
					<img src="https://camo.githubusercontent.com/1a6c6060cac1fe672b424b6cf4e2bc549caf154e6ddbf595f3c597c9ee184497/68747470733a2f2f736c6f632e78797a2f6769746875622f626f797465722f7363632f3f63617465676f72793d636f636f6d6f" style="border: none;" />
					
					<aside class="notes">
					So for Kablamo I work with AWS a lot. So naturally the first through was to use AWS Lambda. After all we know serverless has a lot going for it and should be cheap.

I had previously set this up to produce those github badges you see allowing you to display things like the lines of code on your github project https://github.com/boyter/scc#badges-beta

This worked as follows. I setup a aws lambda function behind a ALB. Calls to it spawn a few subprocesses. The first being a shallow git clone into the tmp location of lambda. https://github.com/boyter/scc-lambda then it runs scc over the checked out code, and uploads the results into s3.

Perfect solution you might think. 

Lambdas have a lot going for them. They are reasonably clean from a developer perspective because you don't have to manage any servers. They scale to thousands and thousands of concurrent requests and they generally don't cost very much.

However there are of course some limitations , but I was prepared to live with. The first is that lambdas behind ALB's or APIGW like this have a hard 29 second timeout. This was not a problem in my case and I was prepared to live with it. 

So I started by exporting all of the git urls, and wrote a simple python script to loop though every repository and ping my URL to generate the the data.

After processing 1 million repositories I then checked my AWS costs and the cost was about $60.

This meant to do all 10 million I was looking at a $700 AWS bill. Ouch. 

I decided to rethink my solution. Keep in mind that was mostly storage and CPU, or what was needed to collect this information. Assuming I processed or exported the data it was going to increase the cost considerably. 
				</section>

				<section>
					<h2><img src="./img/pin.png" style="border: none; background:none; height:0.6em;" />How many “pure” projects</h2>
					<p>
					<img src="./img/languagesPerProject.png" style="border: none;" />
					</p>
					<aside class="notes">
Assuming you define pure to mean a project that has 1 language in it. Of course that would not be very interesting by itself, so lets see what the spread is. As it turns out most projects have fewer than 25 languages in them with most in the less than 10 bracket.

The peak in the graph is for 4 languages.

Of course pure projects might only have one programming language, but have lots of supporting other formats such as markdown, json, yml, css, .gitignore which are picked up by scc. It’s probably reasonable to assume that any project with less than 5 languages is “pure” (for some level of purity) and as it turns out is just over half the total data set. Of course your definition of purity might be different to mine so feel free to adjust to whatever number you like.

What suprises me is an odd bump around 34-35 languages. I have no reasonable explanation as to why this might be the case and it probably warrents some investigation.
					</aside>
				</section>

				<section>
					<h2>Attempt 3</h2>
					<img src="./img/attempt3.png" style="border: none;" /> <br>
					<aside class="notes">Since I was already in AWS the hip solution would be to dump the git repository locations as messages into SQS and pull from it using EC2 instances or fargate for processing. Then scale out the instances like crazy. I actually mentioned this to people at work and thats the solution they also proposed.

Cool, scale out to collect the data. I can do that. 

Collecting the data was one thing, but what about querying it?

The next solution was to use AWS Athena after putting the data into S3. We can then query the data directly in S3 which is cool! Plus its serverless and in theory should be cheap.

Thankfully at the time I was working on a customer project using Athena, and I was able to use that to estimate what it would have cost at the time. So I ran what I expected the data size to be and it was going to cost something like $2.50 USD. Per query! 

Nobody gets their queries right at first and that's half the price of a coffee for a single For that dataset I quickly looked for an alternative.

Also thinking about it, setting up the SQS queue, running instances... its going to be either click-ops development, or a pain to get everything setup properly...
					</aside>
				</section>

				<section>
					<h2><img src="./img/pin.png" style="border: none; background:none; height:0.6em;" />YAML or YML?</h2>
					<table>
					<tr><td>yaml</td><td>3,572,609</td></tr>
					<tr><td>yml</td><td>14,076,349</td></tr>
					</table>
					<aside class="notes">
Sometime back on the company slack there was a “discussion” with many dying on one hill or the other over the use of .yaml or .yml

The debate can finally(?) be ended. Although I suspect some will still prefer to die on their chosen hill.
					</aside>
				</section>

				<section>
					<section>
						<h2>Attempt 4</h2>
						<img src="./img/tacobell.png" height="500px;" style="border: none;" /> <br>
						<aside class="notes">
						To paraphrase the infamous mongodb is web-scale video, as architects and developers, there is a tendency for us to read sites like High Scalability and start thinking we are google architects with google scale problems.

	There is a great post out there on this subject with the title "Taco Bell programming" http://widgetsandshit.com/teddziuba/2010/10/taco-bell-programming.html Its core thesis is that you can solve most problems with the unix tool set, and a bit some judicious use of the pipe operator and redirects. The author gives a wonderful example about a 32 process crawler to download URL's.

	Now I don't subscribe entirely to that idea of doing everything in the shell, probably because I am not at all fluent at sed, but I do think that we need to stop thinking of millions to billions of rows, terabytes of data is a large scale problem. The original map-reduce paper by google came out in 2004 and hadoop came out in 2007. That's over 16 years ago. A lot of the showcases for what those projects could do can be easily done by a single machine these days.

	At the time I had some spare compute used by searchcode. It was fronted by a varnish cache accelerator, and if you have ever used varnish you know its very CPU efficient. As such the CPU on that box was doing the square root of zero most of the time, so I had a look at running the process on it.

	What I did was write a simple Go program that spawned 32 process  which literally called out using spawned processes to shallow git clone to a temp directory then run scc over it, and save the resulting json file to disk and s3.

						</aside>
					</section>
					<section>
						<h2>Attempt 4</h2>
						<img src="./img/hetzner.png" style="border: none;" /> <br>
						<img src="./img/hetzner2.png" height="500px;" style="border: none;" />
						<aside class="notes">
						

						</aside>
					</section>
					<section>
						<h2>Attempt 4</h2>
						<img src="./img/running.png" style="border: none;" /> <br>
						<aside class="notes">
						

						</aside>
					</section>
				</section>

				<section>
					<h2>Processing...</h2>
					<p>
					</p>
					<aside class="notes">The size of the data I needed to process raised another question. How does one process 10 million JSON files taking up just over 1 TB of disk space in an S3 bucket?

Keeping in mind we already ruled out Athena. 

The brains trust at work raised the idea to dump the data into a large SQL database. However this means processing the data into the database, then running queries over it multiple times. Plus the structure of the data meant having a few tables which means foreign keys and indexes to ensure some level of performance. 

This feels wasteful because we could just process the data as we read it from disk in a single pass, assuming we know what we want up front. 

I was also worried about building a database this large. With just data it would be over 1 TB in size before adding indexes.  Now normally I would not worry about this, but I have run into issues with large databases of this size where I spend a fair amount of time turning them in order to get prompt responses.

Probably not a problem if the HDD was a SSD, but for my box it was a mechanical drive.

So I pulled the data out of S3 once and stored each file into a tar file that I could process over and over. This was something I regret having to do because if I have just kept a local version I could probably have processed faster and saved some cost.
					</aside>
				</section>

				<section>
					<h2><img src="./img/pin.png" style="border: none; background:none; height:0.6em;" />The Java FactoryFactory</h2>
					<table>
					<tr><td>not factory</td><td>271,375,574</td><td>97.9%</td></tr>
					<tr><td>factory</td><td>5,695,568</td><td>2.09%</td></tr>
					<tr><td>factoryfactory</td><td>25,316</td><td>0.009%</td></tr>
					<tr><td>factoryfactoryfactory</td><td>0 :(</td><td></td></tr>
					</table>
					<aside class="notes">
Another one that came up in the internal company slack when looking through some old Java code. I thought why not add a check for any Java code that has Factory, FactoryFactory or FactoryFactoryFactory in the name. The idea being to see how many factories are out there.

So slightly over 2% of all the Java code that I checked appeared to be a factory or factoryfactory. Thankfully there are no factoryfactoryfactories and perhaps that joke can finally die, although I am sure at least one non-ironic one exist somewhere in some Java 5 monolith that makes more money every day than I will see over my entire working life.
					</aside>
				</section>

				<section>
					<h2>Raw Numbers</h2>
					<p>
	9,985,051 total repositories<br>
	9,100,083 repositories with at least 1 identified file<br>
	884,968 empty repositories (those with no files)<br>
	3,529,516,251 files in all repositories<br>
	40,736,530,379,778 bytes processed (40 TB)<br>
	1,086,723,618,560 lines identified<br>
	816,822,273,469 code lines identified<br>
	124,382,152,510 blank lines identified<br>
	145,519,192,581 comment lines identified<br>
	71,884,867,919 complexity count according to scc rules<br>
					</p>
					<aside class="notes">
It took about 5 weeks to download and run scc over the collection of repositories saving all of the data. 
This produced just over 1 TB of JSON files containing the results.
It took just over 49 hours to crunch the 1 TB of JSON and produce the results.
					</aside>
				</section>

				<section>
					<h2>Lessons Learnt</h2>
					<p>
					Don't store lots of files in tmp<br>
					Don't use s3 at first...<br>
					Consider compression, suzh as zstd.<br>
					Keep results locally!<br>
					</p>
					<aside class="notes">
					Lessons learnt from this
be careful when creation millions of files in /tmp/ if you ever reboot... because you end up locking your linux box for a long time while it clears those files. 

Ask me how searchcode went down for several hours... I actually ended up having to boot into a recovery mode and then find the fastest way to clear all the files to bring it back online after waiting a few hours for the reboot process to do this

Don't bother with the s3 saving at first, stuff the json into a tar file to some limit, either size or number of files, then compress and shove it into s3

Consider other compression software... gzip is ubiquitous but zstd  is probably the best other option offering better compression and faster performance, and for one off things like this really worth it

If you are going to process outside of s3, be careful storing it there, because the cost to fetch it is really non trivial

For tasks where you throw away the source data such as this, keep the results around locally! Even if you plan on storing them someone having a local copy can really help 
					</aside>
				</section>

		
				<section>
					<h2><img src="./img/pin.png" style="border: none; background:none; height:0.6em;" />Missing a license</h2>
					<img src="./img/hasLicense.png" style="border: none;" />
					<aside class="notes">
This is an interesting one. This was only looking for files with the following names “license”, “licence”, “copying”, “copying3”, “unlicense”, “unlicence”, “license-mit”, “licence-mit” or “copyright”.

So based on that only about 1/3 of repositories have a licence.

This not exhaustive, because the files could have a license via some other means such as SPDX tags, or in the README file. However the results are still problematic.

I have heard some people say they say without a licence is now public domain. I have two issues with this.

The first reason is that its incorrect. You still need to explicitly say its public domain.

The second is that for anyone with Australian or German citizenship (im guessing most of you here), you legally cannot give up your human rights, with copyright being considered a human right. 

So you also cannot put anything into public domain. Germany implemented this right after WW2 so yes its done for a good reason.

So if you want your work to be as open as possible you must be explicit with your license. Consider MIT or Unlicence to have a similar affect. 
					</aside>
				</section>

								

				<section>
					<h2>The Future?</h2>
					<p>
					Keeping the URL properly...<br>
					Forget S3<br>
					Trie to store filenames (avoid lossy)<br>
					Use SQLite....<br>
					Latest version of scc<br>
					Determine maintainability<br>
					Tabs vs Spaces<br>
					</p>
					<aside class="notes">
I am planning on doing this again at some point with the latest version of scc and an updated list of projects from searchcode.
					</aside>
				</section>
		

			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
