<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>GopherConSyd 2023</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<h1>Bloom Filters: Building a Cutting Edge Go Search Engine to Explore the World's Source Code</h1>
					<!--<a href="https://github.com/boyter/indexer">https://github.com/boyter/indexer</a>
					<p>Go get a copy of the above!</p>-->
					<aside class="notes">So as part of the interactive theme, id suggest everyone go and get a copy of the above. Or just go hammer searchcode.com with stuff.</aside>
				</section>

				<section>
					<section>
						<h2>Who are you?</h2>
						<p>
						"Officially" technical lead/principle at Kablamo but a "code monkey" at heart.</p>
						<p>I blog <a href="https://boyter.org/">boyter.org</a> I free software <a href="https://github/boyter/">github/boyter/</a> I run <a href="https://searchcode.com/">searchcode.com</a> also on the twitter <a href="https://twitter.com/boyter">@boyter</a> activitypub <a href="https://honk.boyter.org/boyter">@boyter@honk.boyter.org</a></p>
						<aside class="notes">Not going to bore you too much with who I am because you don't care.
						</aside>
					</section>
					<section>
						<h2>https://boyter.org/</h2>
						<p>Everything is here, now, so feel free to go back to sleep</p>
						<img src="./img/halflisten.jpg" height="500px;" style="border: none;" />
					</section>
				</section>


				<section>
					<section>
						<h2>PHP -> Python -> Java -> Go</h2>
						<img src="./img/sphinx_manticore.png" style="border: none;" />
						<img src="./img/searchcode.gif" height="500px;" style="border: none;" />
						<aside class="notes">
So I run searchcode.com which as the un-imaginative name would suggest searches over source code. 
I have been doing this for the last 10 years modifying and rewriting it from PHP to Python to 
Java and finally to Go.

Searchcode itself indexes about 75 billion lines of code across 40 million projects pulled from 
github, gitlab, bitbucket, codeberg, sr.ht and such.

Its sort of a serious side project, where I get to have a wonderful time working on neat algorithms.
It gets enough traffic to make it interesting. The thing I love is that every part of it from,

index, early termination, syntax highlighting, matching, snippet extraction, 
rate limiting, bot detection, ranking, distributed algorithsm, caching... 
every part is one of those problems you lightly scratch and discover
people getting PhD's in. So there is a lot of width and depth to learn about.

Which is nice compared to the usual code golf I get to deal with. Im sure if I fiddle with this a bit more
I can reduce to code count by a third!

The whole time that has happened one thing has been constant, which was the use of the 
index engine, which was provided using Sphinx Search and then a forked version called Manticore. Both
are great products, and if their capabilities fit into what you need are amazing. I was abusing them
in ways that were causing issue.s

I had been feeling a bit like a fraud, how can I claim to be a search guy without 
using my own index. So after the early in 2020, I decided I would build my own.
						</aside>
					</section>
					<section>
						<p>early termination, syntax highlighting, matching, snippet extraction, 
rate limiting, <strong>index</strong>, bot detection, ranking, distributed algorithms, caching, tokenization,
string searching, regular expressions, data structures, cache lines etc...</p>
					</section>
					<aside class="notes">
As mentioned here are some of the things that go into a search engine.

I could probably talk all day about it but am limited to just the index today. 
					</aside>
				</section>

				<section>
					<h2>Considerations</h2>
					<ul>
						<li>Free service
							<ul>
								<li>Put everything in RAM</li>
								<li>Downtime not a huge issue</li>
								<li>Single server</li>
							</ul>
						</li>
						<li>Single binary, no daemons!</li>
						<li>Boolean queries</li>
 						<li>Single developer, spare time</li>
						<li>This is how I built an index... not necessarily how you should build one!</li>
					</ul>
					<aside class="notes">
Design Considerations
Now we need to consider the goals of searchcode.com
its free... this influences the design, because I don't need to worry about pesky cusotmers, 
if the service goes down
put it all in ram! cos ram is cheap right?
downtime is not a huge problem
single server!
Turns out most people just type terms and expect results, VERY few people actually do boolean queries
Needs to be understandable by me... this is where Go is really useful.
I like the language but I die a little for all the error handling conditions I deal with
Then I realise I finished whatever I was doing really quickly and get over it.
I don't think I would have gotten as far as I did without using Go.

Lastly, this is how I built an index. I have no formal training in it, but as nobody on the internet 
knows you a dog, you don't need permission for a lot of things just action.
					</aside>
				</section>

				<section>
					<section>
							<h2>Searching: Tokenisation</h2>
							<p>Mapping / Stemming</p>
	<pre><code style="font-size: 18px; line-height: 1em;">
"searchcode"         -> [searchcode]
"sydney gophercon"   -> [sydney, gophercon]
"likes liked likely" -> [like]
	</code></pre>
	<p>Does not work for code!</p>
	<pre><code style="font-size: 18px; line-height: 1em;">for(i=0;i++;i&lt;100)</pre></code>
							<aside class="notes">
So code search is in some ways easier than text, because you don't need to try to understand language 
in the way that you do when dealing with english, japanese or russian. Splitting on chinese or japanese 
words for example is painful.
							</aside>
					</section>
					<section>
						<h2>Searching code: Tokenisation</h2>
						<p>Trigrams</p>
<pre><code style="font-size: 18px; line-height: 1em;">"searchcode"   -> [sea, ear, arc, rch, chc, hco, cod, ode]
"pppppppppppp" -> [ppp]
</code></pre>
						<aside class="notes">
You can solve this problem in code using whats known as trigrams, where you break the text apart into c
haracters of 3 and index those. You can actually use any number you want for ngrams, 
but trigrams work well when indexing source code.

One catch with this is that you get false positives, some inputs generate the same 
ngrams despite being different. You also have terms with low cardinality such 
as repeated works in a line. So the letter p repeated 100 times is no more useful than 3 of them. 
They map to the same thing

But have a look here, were one term searchcode is turned into 8 terms we need to index.
						</aside>
					</section>
					<section>
						<h2>Problems</h2>
						<p>Creates long posting lists</p>
						<p>Introduces false positives</p>
<pre><code style="font-size: 18px; line-height: 1em;">"searchcode" -> [searchcode]</code></pre>
						<p>vs</p>


<pre><code style="font-size: 18px; line-height: 1em;">"searchcode" -> [sea, ear, arc, rch, chc, hco, cod, ode]
</code></pre>
						<aside class="notes">
If we tokenise by trigrams we get a great deal more tokens we need to index. In our example 
searchcode turns into 8 terms we need to add to our index, vs a single thing we would need if we treated
it as plain space delimited text. This is an important consideration, because it enlarges the index a great deal.
						</aside>
					</section>
					<section>
						<h2>Trigram Generation<h2>
<pre><code style="font-size: 14px; line-height: 1em;">func Trigrams(text string) []string {
	var runes = []rune(text)

	if len(runes) <= 2 {
		return []string{}
	}

	ngrams := make([]string, len(runes)-2)

	for i := 0; i < len(runes); i++ {
		if i+3 < len(runes)+1 {
			ngram := runes[i : i+3]
			ngrams[i] = string(ngram)
		}
	}

	return ngrams
}
</code></pre>
					<aside class="notes">
So here is some code to create trigrams given some input text.

Interesting fact, this is by far the single biggest bottleneck in searchcode when indexing.
I have tweaked this a great deal.
If someone wants to profile and improve performance on it I would really appreciate it. 
It dominiates the indexing profile by a huge amount.</aside>
					</section>
					<section>
					<img src="./img/nerdsniped.png" style="border: none;" />
					<aside class="notes">
Consider yourself nerd sniped.
					</aside>
				</section>

				</section>

				

				<section>
					<section>
						<h2>What is an index?</h2>
						<p>Structured list. Keys point at data so searches are faster.</p>
						<p>Given a term, return id's which contain it.</p>
						<aside class="notes">
So what is an index. The definition can be fuzzy, but in my case I treat it as a 
function that I pass in a term or terms to search for and it returns a list of integers
which refer to documents that likely contain the terms, possibly ordered by relevance.
						</aside>
					</section>
					<section>
						<h2>Textbook 101 index</h2>
						<p>Average CS textbook example</p>

<pre><code style="font-size: 14px; line-height: 1em;">package main

func main() {
	// non-positional index
	index := map[string][]int{} 
	index["gopher"] = append(index["gopher"], 1337)

	// positional index
	type pos struct {
		id  int   // unique document id
		loc []int // store document positions in posting list
	}
	posIndex := map[string][]pos{} 
	posIndex["gopher"] = append(posIndex["gopher"], pos{1337, []int{1, 2, 3, 99}})
}
</code></pre>

						<aside class="notes">
So given our trigram terms we now need to index them. There are a few ways to do this. Lets walk through a few.

A posting list is the slice of struct or int's containing the document's that contain that term.

So "gopher" is found inside documents 1337 here.

A positional index is one that stores where the term was found in the document. 

So "gopher" as position 1, 2, 3 and 99.

Positional index are really useful for phrase searches and ranking.
In theory you can then use them to reconstruct the document entirely. 
One catch with this is that your index tends to becomes as large or larger than the thing you are indexing.

Done simply as presented here these are a bit of code you can had over to your cousin or most junior developer.
						</aside>
					</section>
					<section>
						<h2>Problem: Intersect two lists</h2>
						<img src="./img/assvogel.gif" height="500px;" style="border: none;" />
						<ul>
							<li>Trigrams create LONG posting lists</li>
							<li>Need to implement skip lists AND compression at scale</li>
						</ul>

						<aside class="notes">
The big problem with any approach that uses posting lists is that they get long.

Consider the search for "the assvogel". The is a very common word, and appears in nearly every document.
Assvogel is very rare and only appears in a few.

To get the intersection of them you have to walk the "the" posting list for a long time to find a match.
This gets worse when you add more common terms.

Skip lists are something you end up needing to implement because when searching multiple terms you look for 
the intersection of multiple posting lists and it speeds things up and something you need to add to scale this approach.

They are an interesting data structure worth looking up on wikipedia. When you start adding compression using ellias phano to reduce the size they quickly become complex, and it becomes easy to tank your performance.

						</aside>
					</section>
					<section>
						<h2>For those wondering...</h2>
						<img src="./img/vulture.jpg" style="border: none;" />
						<aside class="notes">
Assvogel is an obsolete Afrikaans word for a South African vulture. Today you learnt.
						</aside>
					</section>
					<section>
						<h2>Problem 2</h2>
						<h3>Compression</h3>
						<pre><code style="font-size: 14px; line-height: 1em;">index := map[string][]int{} // this should be compressible right?</code></pre>
						<p>Shannon Elias Fano</p>
						<img src="./img/elias_fano.png" style="border: none;" />
						<aside class="notes">
Elias Fano compression is a technique used to compress a sorted list of integers.

Great thing to implement, not so much to explain. Lets just assume its magic.
						</aside>
					</section>
					<section>
						<h2>Complexity</h2>
						<pre><code style="font-size: 16px; line-height: 1em;">index := map[string][]int{} // non-positional index
</code></pre>
<p>vs</p>
						<img src="./img/complex.png" style="border: none;" />
						<aside class="notes">
This is clearly a lot more complex than what we started with. 

Certainly more than I was willing to deal with over time.

Trust me you don't want me writing algorithms like this.
						</aside>
					</section>
					<section>
						<h2>My attempt</h2>
						<p>Two turkeys duct taped together does not make an eagle.</p>
						<img src="./img/turkeys.jpg"  height="500px;" style="border: none;" />
						<aside class="notes">
So I did attempt to create the CompressedSkipList.

I implemented both the skip list and elias fano compression, and got two turkeys duct taped together.
Not the eagle I wanted. So I rethought my approach.
						</aside>
					</section>
					
				</section>

				<!--<section>
					<h2>Trie</h2>
					<p>Typesense uses this, its also written in C++</p>
<img src="./img/trie.png" height="100px" style="border: none;" />
					<ul>
						<li>Big problem is not friendly to GC due to the use of pointers</li>
						<li>Still need skip lists + compression on the posting lists</li>
 						<li>Makes wildcard queries easier</li>
					</ul>

					<aside class="notes">
I tried this, and the GC non friendly problem caused it to have massive delays while walking the pointers. 
I think it was seconds in GC when I tried it which was unacceptable.

I could have possibly done off heap tricks but I wanted to stick to normal Go because I am a mortal programmer.

So what about bitslice signatures?
					</aside>
				</section>-->

				<section>
					<section>
						<h2>How about bloom filters?</h2>
						<p>A.K.A Bitsliced signatures</p>
						<aside class="notes">
There is a technique to search using bloom filters. However it helps if you know what one is.
						</aside>
					</section>
					<section>
						<h2>Bloom filter: Empty</h2>
						<p>16 boolean's in an array</p>
	<img src="./img/BloomFilter1.png" style="border: none;" />
						<aside class="notes">
In short a probablistic data structure, that you can use to test the existance of something. 
You add items, and can check if it was added. 

They never return false negatives, but they do occassionaly lie and report something being added when it was not.

So lets look at this example,

We have a 16 bit bloom filter. Its just a slice of booleans set by default to false, indicated by 0 here.
						</aside>
					</section>
					<section>
						<h2>Bloom filter: Add</h2>
						<p>Hash the term 3 times and set the bits</p>
	<img src="./img/BloomFilter2.png" style="border: none;" />
						<aside class="notes">
To add something to the filter we get a term, hash it using a integer returning hash like fnv, and use the resulting
outputs to map to bit positions inside our filter. We then set those positions to true.
						</aside>
					</section>
					<section>
						<h2>Bloom filter: Add second</h2>
						<p>3 more bits set</p>
	<img src="./img/BloomFilter3.png" style="border: none;" />
						<aside class="notes">
Add another term. Same process, hash 3 times and set the bits.
						</aside>
					</section>
					<section>
						<h2>Bloom filter: Add overlapping bits</h2>
						<p>"big" and "dog" share 2 bits</p>
	<img src="./img/BloomFilter4.png" style="border: none;" />
						<aside class="notes">
Another term added, where 2 of the bits overlap. This is an expected property of the filter
and one of the ways they are space efficient.
						</aside>
					</section>
					<section>
						<h2>Bloom filter: Hit</h2>
						<p>hash "big" and check bits</p>
	<img src="./img/BloomFilter5.png" style="border: none;" />
						<aside class="notes">
To check if the filter has something we do the same process. Hash the term as we did on insert,
then check the bit positions. They are all true so this indicates big was probably added to the filter.
Why probably? We will get to that in a moment.
						</aside>
					</section>
					<section>
						<h2>Bloom filter: Miss</h2>
						<p>one bit position is 0 so miss</p>
	<img src="./img/BloomFilter7.png" style="border: none;" />
						<aside class="notes">
Another check, but notice that the middle bit is false. This means we know that sydney was never added
to this filter.
						</aside>
					</section>
					<section>
						<h2>Bloom filter: False Positive</h2>
						<p>big and yellow supplied bits</p>
	<img src="./img/BloomFilter6.png" style="border: none;" />
						<aside class="notes">
Here we see an example of a false positive match. We hash and check the positions and they are all true.
But we never added this term, the bits were supplied by other terms.

While bloom filters do introduce false positives, you have to deal with them anyway since we are using trigrams.
So this is not a deal breaker.

A lot of tweaking of bloom filters is to control the false positive rate, where higher or lower values can be useful.
						</aside>
					</section>

					<section>
						<h2>Go Bloom Filter</h2>

<pre><code style="font-size: 14px; line-height: 1em;">package main

import (
	"fmt"
	"hash/fnv"
)

func main() {
	bloom := make([]bool, 16)          // 16 bit bloom filter
	hash := func(term string) uint64 { // single hash function for bloom filter
		hsh := fnv.New64()
		_, _ = hsh.Write([]byte(term))
		return hsh.Sum64() % uint64(len(bloom))
	}

	bloom[hash("gopher")] = true // add to the filter
	bloom[hash("sydney")] = true

	for _, i := range bloom { // print out the filter
		if i == true {
			fmt.Print("1")
		} else {
			fmt.Print("0")
		}
	}

	if bloom[hash("sydney")] == true { fmt.Print("\nprobably added") }
	if bloom[hash("house")] == false { fmt.Print("\nwas not added") }
	if bloom[hash("boyter")] == true { fmt.Print("\nfalse positive! was never added!") }
}
</code></pre>

<pre><code style="font-size: 14px; line-height: 1em;">$ go run main.go 
0010000001000000
probably added
was not added
false positive! was never added!
</code></pre>

						<aside class="notes">
So here is a bloom filter implemented in go. The filter itself is 4 lines of code. A slice of booleans,
a function to hash and thats all you need. The output shows that terms were added, and a false positive.
						</aside>
					</section>

					<section>
						<h2>Bloom filter: search</h2>
					
						<p>Check bit positions 1 and 7. Document 4 matches.</p>
						<pre><code style="font-size: 14px; line-height: 1em;">for each bloomfilter
	for each bit
		check if bit location in filter is set
	if all matching bits are set
		record possible match</code></pre>

						<img src="./img/bloom_naieve_search.gif" height="400px" style="border: none;" />

						<aside class="notes">
So how to search over out bloom filter index? 

You just add more bloom filters.

This represents 4 documents in our index with their own filter. 
All indexed using an 8 bit bloom filter. 

So to search we hash out search terms, which come to position 1 and 7. 
We loop over each filter, checking the bit positions at 1 and 7.
If any are false we know it cannot be a match.
If all are true we have a possible match. In this case document 4.
						</aside>
					</section>
				</section>


				<!--<section>
						<h2>Frequency Concious Bloom Filter</h2>

					<aside class="notes">So hashing... While you can has terms a single time in a bloom filter, it turns out that you can reduce the false positive rate by having multiple hashes. If you do this dependant on the term input you end up with a Frequency Conscious Bloom Filter.

The reason is that rare terms need more hashes to avoid false positives.

To get the frequency for searchcode I just calculated the hash counts for every bit of code I found find and removed all the common ones. I then use a weight to determine how many hashes each term needs.

Its left as an exercise to yourselves to implement this, as I just hardcoded it to 3 hashes here.

BTW this is one of the things you need to do at a large enough scale to get the performence you need and is something I put into searchcode.</aside>
				</section>-->

				<section>
					<section>
						<h2>Advantages</h2>
						<ul>
							<li>Compressed. Only using several bits per term!</li>
							<li>Very simple!!!!
								<ul>
								<li>Adding</li>
								<li>Editing</li>
								<li>Searching</li>
								<li>Extensible</li>
								</ul>
							</li>
						</ul>
						<aside class="notes">
Bloom filters have a lot going for them. They are compressed by nature, getting down to 9 bits per term 
added if you do them right. 
They are also really simple to implement. 
Adding is simple, just add a new filter.
Editing is simple, just set some booleans or replace the filter.
Searching is a really simple for loop.
You can extend them using counting bloom filters or probably even positions.
						</aside>
					</section>
					<section>
						<h2>Problems</h2>
						<p>No free lunch...</p>
<pre><code style="font-size: 14px; line-height: 1em;">package main
			
import (
	"fmt"
	"runtime"
)

func main() {
	memUsage := func() string {
		var m runtime.MemStats
		runtime.ReadMemStats(&m)
		return fmt.Sprintf("%v MB", m.Alloc/1024/1024)
	}

	fmt.Println(memUsage())
	bigBloom := make([]bool, 100_000_000) // represents lots of bloom filters in memory
	fmt.Println(memUsage())
	_ = len(bigBloom)
}
</code></pre>

<pre><code style="font-size: 14px; line-height: 1em;">$ go run main.go
0 MB
95 MB</code></pre>
						<aside class="notes">
There are of course problems.

Firstly.

How much memory does 100 million bools take up? Almost 100 MB which if you calculate is about 8x what you expect.

This program shows the issue nicely.

Problem is that each "bit" in this uses 1 byte under the hood since they are all addressable.
Not a problem with Go it occurs in any other major language.
						</aside>
					</section>
					<section>
						<h2>Problem 2</h2>
						<p>Generic RAM stick</p>
						<img src="./img/ram.jpg" height="500px" style="border: none;" />
						<aside class="notes">
Another problem is that is that its slow... This comes down to how fetching bits out of 
memory using the CPU works.

When you probe a single bit on most modern sysems you actually pull back 512 bits from RAM. This is because a 
full cache line is read, which is 512 bits or 64 bytes. So its a 512x overhead just to test a single bit! 

As a result on average you end up walking ALL the memory for your filter, depending on which bits your probe.

This is one one of the reasons this as an approach fell out of favor in the 70's, because in practice
you do end up walking all the addressable memory.
						</aside>
					</section>
					<section>
						<h2>Illustration</h2>
						<img src="./img/2048_bloom_2.png" height="500px" style="border: none;" />
						<aside class="notes">
Here is a representation of the issue. We have a 2048 bit bloom filter, where each square is a bit.
We have hashed our terms to 6 bit positions.

We used a good hash function so we get a decent distribution within those locations and now want to probe those
bits.
						</aside>
					</section>
					<section>
						<h2>Can we do better?</h2>
						<img src="./img/2048_bloom_3.png" height="500px" style="border: none;" />
						<aside class="notes">
Because the bits are all within 512 bits of each other, we are likely to pull the entire filter across 
the CPU.

This is a big problem because accessing memory while fast isnt as fast as you expect. Modern CPU's can theoriticly 
push 100 GB across the CPU per second. In reality you are more likely to get 50 GB. So if your index is 100 GB in size
you are limited to 1 search every two seconds. 

Can we do better?
						</aside>
					</section>
				</section>

				
				

				<section>
					<section>
						<h2>Bitfunnel</h2>
						<p>Good enough for Dan Luu? Good enough for you.</p>
						<img src="./img/bitfunnel.png" style="border: none;" />
						<aside class="notes">
In 2017 as SIGIR this paper about bitfunnel was released, detailing how bing uses
bit signatures or bloom filters to query within the index. I believe at the time
it was just searching the fresh index, but powers everything now.

It did win best paper at SIGIR, which is probably the most prestigious IR conferences.

Dan Luu is a co-author. Anything he writes I pay attention to.
Hence my saying,  "Good enough for Dan Luu, Good enough for you."
						</aside>
					</section>
					<section>
							<h2>Fixes</h2>
							<p>Rotate the filter. Documents now on columns not rows.</p>
							<img src="./img/rotate.gif" height="500px" style="border: none;" />
							<aside class="notes">
So lets start.

In the 80's a smart person called Roberts noticed you could rotate the filter, 
assuming each filter has the same exact size, IE the same number of bits.

So we do that by turning the rows into columns. So each row used to represent a document, but now each column does. Right to left.

With this done we only need to inspect the rows containing the bit positions of the query. 
		</aside>
					</section>
					<!--<section>
						<p>Rotated filter (documents are columnns now)</p>
						<img src="./img/rotate1.png" height="500px" style="border: none;" />
						<aside class="notes">
						</aside>
					</section>-->
					<section>
						<p>Fetch row 1 and 7 same as previous example</p>
						<img src="./img/rotate3.png" height="500px" style="border: none;" />
						<aside class="notes">
So in this case 2 rows which is half the memory access.

We can then using out examples just row query bit positions 0 and 6 and then logically & them together. If they arent 0 then we have a positional match.

						</aside>
					</section>
					<section>
						<p>Logically & all rows</p>
						<img src="./img/rotate4.png" height="500px" style="border: none;" />
						<aside class="notes">
The nice thing about this is that a & in the CPU is as close as we get to free operation from a performance point of view.
In fact the computation for this is so low it beomes all about the memory lookups.
						</aside>
					</section>
					<section>
						<p>Pos 1 is true, so document 4 matches</p>
						<img src="./img/rotate5.png" height="500px" style="border: none;" />
						<aside class="notes">
So after the logical & we see a single bit is left on in the first position with corresponds to document 4.
This is because we filled right to left, based on our rotation.
						</aside>
					</section>
					<section>
						<h2>Results?</h2>
						<p>This reduces the amount of RAM we need to access by a huge factor for larger bloom filters.</p>
						<aside class="notes">
A huge factor. Something like 200x. We still need to be careful because of cache lines, but there are further tricks.
						</aside>
					</section>

					<section>
						<h2>Pack the bits with bit set</h2>
						<p>Use int64's to hold the filters in columns, and flip bits starting right to left.</p>
<pre><code style="font-size: 14px; line-height: 1em;">var bloomFilter []uint64
var bloomSize = 16
var currentBlockDocumentCount = 0
var currentBlockStartDocumentCount = 0

func Add(item []bool) error {
	if currentBlockDocumentCount == 0 || currentBlockDocumentCount == 64 {
		bloomFilter = append(bloomFilter, make([]uint64, bloomSize)...)
		currentBlockDocumentCount = 0
		currentBlockStartDocumentCount += bloomSize
	}

	for i, bit := range item {
		if bit {
			bloomFilter[currentBlockStartDocumentCount+i] |= 1 << currentBlockDocumentCount
		}
	}

	currentBlockDocumentCount++
	currentDocumentCount++

	return nil
}
</code></pre>
					<aside class="notes">
So lets write code to pack our filters. This code packs in one bloom filter at a time into int64's
right to left, and once hitting document 65 adds more empty ints so we can continue adding documents.

As a result to add documents we need only turn them into trigrams, hash those into a normal bloom filter,
and pass them into this. Very simple from a code perspective.
					</aside>
					</section>




					<section>
						<h2>Result</h2>
						<p>16 bit bloom filter, with 32 documents added. Less wasted space.</p>
<pre><code style="font-size: 14px; line-height: 1em;">bloomFilter := make([]int64, 16) 
</code></pre>
<pre><code style="font-size: 14px; line-height: 1em;">0000000000000000000000000000000010100011111111111111111111111101
0000000000000000000000000000000001110000000000010000100000000000
0000000000000000000000000000000000000000000000000000000000000000
0000000000000000000000000000000000100011111100000000000000000000
0000000000000000000000000000000000100000000000000000000000000000
0000000000000000000000000000000011000000000000000000000000000000
0000000000000000000000000000000000000000000000000000000000000000
0000000000000000000000000000000000001000000000000001000000000000
0000000000000000000000000000000000111100000000000000000000000000
0000000000000000000000000000000000100001111100000001010101011110
0000000000000000000000000000000010000010000010001010101010100011
0000000000000000000000000000000001011000000000000000000000000000
0000000000000000000000000000000001101011111111111111111111111111
0000000000000000000000000000000000110000001000000001100000001000
0000000000000000000000000000000010100010000111111110101010100001
0000000000000000000000000000000001100011111100000000000000000000
</code></pre>
						<aside class="notes">
This is what it looks like in memory. We have a 16 bit bloom filter, with 16 int64's one after the other.
We have added 32 documents to it, so only the right most 32 bits have anything set. 
If we added another 32 documents the whole thing would look like the right side.

Logically I call these a block or bucket.

This ensures we are optimally using the space. Because we pack 64 documents into each 
bucket we end up using a single bit per location in the bloom filter. No 8x waste!

It also becomes very fast to iterate this in code because its a simple for loop over a slice. 

We all know for loops go brrrr.
	</aside>
					</section>
			
					<section>
						<h2>Sharding</h2>
						<p>Shard based on the length of the document we want to index.</p>
<pre><code style="font-size: 14px; line-height: 1em;">0000000100000000 // underfilled bloom filter
1011111111111110 // overfilled bloom filter</code></pre>
						<p>Result? Less wasted space. Squeeze every bit.</p>
<pre><code style="font-size: 14px; line-height: 1em;">
┌──────────────────┐   ┌──────────────────┐   ┌──────────────────┐
│     caisson      ├─┬▶│  shard 512 bits  ├┬─▶│ bucket-1 64 docs │
└──────────────────┘ │ └──────────────────┘│  └──────────────────┘
                     │                     │                      
                     │                     │  ┌──────────────────┐
                     │                     ├─▶│ bucket-2 64 docs │
                     │                     │  └──────────────────┘
                     │                     │                      
                     │                     │  ┌──────────────────┐
                     │                     └─▶│ bucket-3 64 docs │
                     │                        └──────────────────┘
                     │                                            
                     │                                            
                     │ ┌──────────────────┐   ┌──────────────────┐
                     └▶│  shard 1024 bits ├┬─▶│ bucket-1 64 docs │
                       └──────────────────┘│  └──────────────────┘
                                           │                      
                                           │  ┌──────────────────┐
                                           └─▶│ bucket-2 64 docs │
                                              └──────────────────┘
</code></pre>
						<aside class="notes">
However the fix is still a little wasteful because smaller documents don't need large filters.

So we shard on the document length, where larger documents get larger bloom filters.

	In searchcode its a little more in depth because it actually changes the size of the bloom filters 
	based on the size of the document its indexing. Note I re

	What happens is when a document is indexed, it breaks it into unique trigrams, and then finds a 
	bloom filter where it will be stored with a target bit density. If the document has a small amount 
	of trigrams it ends up in a smaller bloom filter of say 256 bits, and if larger it goes into a 
	larger one. This is done to avoid over filling the filters, which drives up the false positive rate,
	or under filling them and wasting memory.

	When the full 64 documents have been added to a bucket then in the case of a 512 bit bloom filter another 512 
	int64's are appended on the end, and then those are filled right to left.

	Where shards represent a different bloom filter size, say 512 of 2048 bits, and buckets exist as a 
	single uint64 slice in a shard. They are split out logically here to help understanding.
						</aside>
					</section>
					<section>
						<h2>Copy the shards after indexing</h2>
						<p>Result? Less fragmented memory, faster search.</p>
<pre><code style="font-size: 14px; line-height: 1em;">bloomFilter = append(ci.bloomFilter, make([]int64, 512)...)

// attempt to save/organise memory shrink lists... https://go.dev/blog/slices-intro#TOC_6
out := make([]int64, len(bloomFilter))
copy(out, bloomFilter)
bloomFilter = out
</code></pre>
					</section>
					<section>
						<h2>Frequency Concious Bloom Filter</h2>
						<p>Rare terms need more hashes to reduce false positive rate</p>

<pre><code style="font-size: 14px; line-height: 1em;">func (ci *CaissonIndex) DetermineHashCount(ngram string) int {
	// version 0.1
	// if nothing that indicates its a very rare term so it needs the most hashes
	// so set that up as the default
	hashCount := 5

	v, ok := ci.termTreatments[ngram]
	if ok {
		weight := float64(v) / float64(ci.highestTermCount) * 100

		if weight >= 10 {
			hashCount = 1
		} else if weight >= 5 {
			hashCount = 2.5
		} else if weight >= 2 {
			hashCount = 3
		} else if weight >= 1.5 {
			hashCount = 4
		}
	}

	return hashCount
}
</code></pre>
					</section>

					<section>
						<h2>Result?</h2>
						<p>Wasted time... For trigrams anyway</p>
						<p>New algorithm is mind blowing.</p>
<pre><code style="font-size: 14px; line-height: 1em;">func DetermineHashCount(ngram string) int {
	return 2
}
</code></pre>
						<aside class="notes">
Amazing I know. I found this out by testing different hash counts from 1 to 10 against the frequency ones
against a dozen different sized bloom filters looking for the lowest false positive rate.

In almost every case 2 worked out to give the best possible signal to noise ratio. I suspect this is due to
trigrams being fairly common across all code with none being particiually unique unlike natural language.
						</aside>
					</section>

					<section>
						<h2>Restrict Parallelism</h2>
						<p>Parallelism works... if you have CPU to spare</p>
<pre><code style="font-size: 14px; line-height: 1em;">var sem = make(chan bool, 5) // counting semaphore

func doSearch() { // only 5 instances of this function can run
	sem <- true
		defer func() {
		<- sem
	}()

	// Do CPU/Memory intensive stuff here
}
</code></pre>
						<aside class="notes">
This one seems counter intuitive. 

But the thing is when it comes to processes that are really pushing system limits, doing things in serial
to an extent can be faster. 

For search, imagine each query takes one second. Process in parallel and if 10 requests come in, they all
get a result 10 seconds later. Do it in serial and one user gets it in 1 second, the second in 2 etc.. to 10.

This is where I really like Go functionality. I remember writing a counting semaphore in C# once and it was
a horrible bug ridden thing. To have something so simple in Go using using channels is amazing language design.
						</aside>
					</section>
					<section>
						<h2>Searching: Done per shard</h2>
						<p>I find this algorithm beautifully simple.</p>
<pre><code style="font-size: 14px; line-height: 1em;">func Search(queryBits []uint64) []int {
	var results []int
	var res uint64

	for i := 0; i < len(bloomFilter); i += 2048 {
		res = bloomFilter[queryBits[0]+uint64(i)]

		for j := 1; j < len(queryBits); j++ {
			res = res & bloomFilter[queryBits[j]+uint64(i)]

			if res == 0 { // important! skip shard if nothing!
				break
			}
		}

		if res != 0 {
			for j := 0; j < 64; j++ {
				if res&(1<&lt;j) > 0 {
					results = append(results, 64*(i/2048)+j)
				}
			}
		}

	}

	return results
}
</code></pre>
						<aside class="notes">
With the index done we can now search.
					
The core search algorithm is as follows. 

I find this algorithm beautifully simple. We take in query positions for the search, so in the case of searching for "searchcode" we turn that into trigrams
then hash them using the same hash functions to get 6+ uint64's from 0 to whatever the filter is configured to be in a slice.

Then, walk over each logical block or bucket, which is as many uint64's as exist in the bloom filter, and then logically & that against the previous one. 
If its 0 bail out, otherwise if we have finished record the documentid based on what its position in the index is.


Whats really cool about this is that we jump potentially all of the bytes in a block if we hit that res == 0 condition quickly, avoiding us walking any memory at all! This is the state we want
to hit as often as possible.

Results when returned gives ids, which we can then use the idToFile to find the true id for this document. 

Once we have these id its a matter of reaching out to our primary data store, pulling back the documents and processing from there.
	</aside>
					</section>
					<section>
						<h2>Searching: Visually</h2>
						<p>Perform & between each row, and if we see 0 skip to next bucket</p>
						<img src="./img/bloomsearch.gif" height="500px" style="border: none;" /><br>
					</section>

					

					<section>
						<h2>Grand Total Result</h2>
						<p>Rolling average index search time: ~50ms</p>
						<p>140,000,000 to 180,000,000 files</p>
						<p>~100GB RAM</p>
						<img src="./img/searchtime.png" style="border: none;" />


<pre><code style="font-size: 14px; line-height: 1em;">{
    "level": "info",
    "totalCount": 266,
    "falseCount": 144,
    "positiveCount": 122,
    "timeMillis": 24,
    "reductionPercent": 0.4586466165413534,
    "query": "while (*d++ = *s++); source:github repo:linux",
    "time": "2023-11-08T06:30:19+01:00",
    "message": "search"
}</code></pre>

					</section>
				</section>
					
				<section>
					<h2>Conclusions</h2>
					<p>Go. I doubt I could have done it in another language. I still don't like nil checks.</p>
					<p>Bloomfilters + Trigrams works well. As far as I know this is globally unique. Nobody else I know uses this approach.</p>
					<p>Dealing with false positives from the filters isnt a deal breaker due to trigrams.</p>
					<aside class="notes">As you can see its a short bit of code. Only about 150 lines. I really encourage you to have a look and play with it.</aside>
				</section>

				<section>
					<h2>Thank You!</h2>
					<p>Probably nobody cases how searchcode works, but I care, and I know. Now you do too.</p>
					<img src="./img/gopher.png" height="500px" style="border: none;" /><br>
					<a href="https://searchcode.com/">https://searchcode.com/</a><br>
					<a href="https://boyter.org/posts/how-i-built-my-own-index-for-searchcode/">https://boyter.org/posts/how-i-built-my-own-index-for-searchcode/</a><br>
					<a href="https://boyter.org/">https://boyter.org/</a>
					<aside class="notes">
Of course... probably nobody using searchcode probably cares that its running on a unique bloom filter trigram
backed index with ideas borrowed from bing. Even if I am fairly sure this is the only instance of any search engine doing so.

But I know... and now you do too.

Now if you want one of these great looking stickers designed in house at Kablamo, 
come talk to myself or anyone else at the Kablamo desk and get one. Feel free to tell me 
all the mistakes I made too.
					</aside>
				</section>

			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
