<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Abusing Go, AWS Lambda and bloom filters to make a true Australian serverless search engine</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<h1>Abusing Go, AWS Lambda and bloom filters to make a true Australian serverless search engine</h1>
					<aside class="notes">
Hello everyone, I hope you are having a wonderful day.
Sorry about the clickbait title, you need to standout in the selection process somehow. Its how I compensate
for lack of skill.
					</aside>
				</section>

				<section>
					<section>
						<h2>Who are you?</h2>
						<p>
						"Officially" technical lead/principal at Kablamo but a "code monkey" at heart.</p>
						<p>I blog <a href="https://boyter.org/">boyter.org</a> code free software <a href="https://github/boyter/">github/boyter/</a> I run <a href="https://searchcode.com/">searchcode.com</a> also on the x.com <a href="https://x.com/boyter">@boyter</a> activitypub <a href="https://honk.boyter.org/boyter">@boyter@honk.boyter.org</a></p>
						<aside class="notes">
So who am I? 
I am not going to bore you too much with who I am because you probably don't care. 
Im not famous, but you can find me at these places.
						</aside>
					</section>
					<section>
						<h2>https://boyter.org/</h2>
						<p>Everything is <a href="https://boyter.org/">there</a>, so feel free to go to sleep</p>
						<img src="./img/halflisten.jpg" height="500px;" style="border: none;" />
						<aside class="notes">
For those who prefer to hang out on slack everything you are about to see is now available boyter.org
Don't worry too much about missing anything.
You can even skip ahead on the slides and head off for a coffee if you like. I won't judge you.
						</aside>
					</section>
				</section>

				<section>
					<section>
						<h2>My hobby; search engines</h2>
							<p>early termination, syntax highlighting, matching, snippet extraction, 
	rate limiting, index, bot detection, ranking, distributed algorithms, caching, tokenization,
	string searching, regular expressions, data structures, line detection, duplicate detection, 
	literal extraction, unicode, case insensitive matching etc...</p>
	<aside class="notes">
	So my hobby is building search engines.

	Here are some of the things that go into any search engine.

	Every part you see is one of those problems you lightly scratch and discover
	people getting PhD's in. So it constantly keeps me engaged and learning.
						</aside>
					</section>
					<section>
						<h2>searchcode.com</h2>
						<img src="./img/searchcode.gif" height="500px;" style="border: none;" />
						<aside class="notes">
So a few years ago I was building a new index for searchcode.com from scratch which runs on a dedicated server.
I mentioned this to a work colleague and he 
asked why I didn't use AWS as generally for work everything lands there. 
I mentioned something to the effect that you needed a lot of persistent storage, 
or RAM to keep the index around which is prohibitively expensive on AWS. 

He mentioned perhaps using Lambda? And I responded its lack of persistance is a problem.
						</aside>
					</section>

					<section>
						<h2>Lambda/Serverless Weaknesses</h2>
						<ul>
							<li>Persistance</li>
							<li>Performance</li>
						</ul>
						<aside class="notes">
Lamba's or any other serverless function work well for certain problems. 
So long as you can rebuild state inside the lambda, or respond to events they work well.
The lack of persistance is an issue in search because
you either store the index in RAM, or on disk.

So I was explaning this to this collegue and I trailed off. Something occurred to me.

There is a saying in computing.
						</aside>
					</section>

					<section>
						<blockquote>Never do at runtime what you can do at compile time.</blockquote>
						<aside class="notes">
I decided to see how far I can take that idea, by using AWS Lambda to build a search engine. 
						</aside>
					</section>

					<section>
						<h2>The idea!</h2>
						<p>Compile the index INTO the lambda and deploy that</p>
							<ul>
								<li>50 MB zipped binary</li>
								<li>75 GB of space to store all your lambda's</li>
								<li>AWS Lambda 1,000,000 free requests per month</li>
								<li>AWS Lambda 400,000 seconds of compute time per month</li>
							</ul>
						<aside class="notes">
Some see constraints. I see free compute.

We get around the pack of persistance? By baking the index into the lambda themselves. 
In other words, generate code which contains the index and compile that into the lambda binary. 
Do it at compile time.

The plan, is then to shard the index using individual lambda's. 
Each one holds a portion of the index compiled into the binary that we deploy. 

This means the entire thing is 100% serverless.

The best part about this is that it solves one of the big problems with building a search engine. 
That problem is that you need to pay for a heap of machines to sit there doing nothing up till someone 
wants to perform a search.

It will probably slide under the AWS Lambda Free tier.
						</aside>
					</section>
				</section>



				<section>
					<section>
						<h2>Proving the theory</h2>
						<p>Brute Force</p>
						<pre><code style="font-size: 14px; line-height: 1em;">package main

func main() {
	index := []string{
		"Memcached vs Redis - More Different Than You Would Expect",
		"You Don't Need a Library for File Walking in Go",
		// ... 99,997 more ...
		"Lessons Learnt Building for the Atlassian Marketplace",
	}

	for _, x := range index {
		strings.Contains(x, "searchterm")
	}
}
</code></pre>
						
						<aside class="notes">
First thing I wanted to do was prove the theory. I started with a simple brute force search.

Brute force is a viable strategy, assuming you have a lot of force.

Tools such as grep can walk gigabytes of content in seconds so this isnt a bad idea in theory.

Taking a guess of storing ~100,000 items in a lambda, a modern 
CPU string searching in memory should return in a few hundred milliseconds.

So I tried it. I created a Go file with 100,000 strings in a slice, and then wrote a simple loop 
to run over that performing a search.
						</aside>
					</section>
					<section>
						<h2>This does not work...</h2>
						<p>Several seconds per search...</p>
						<img src="./img/lambdacpu.png" style="border: none;" />
						<aside class="notes">
Alas I underestimated how weak the CPU allotted to a lambda is, and searches took several seconds. 
Even increasing the RAM to improve the CPU allotment didn't really help. 

My fallback plan was to embed an index into the lambda, allowing for a quick scan of the documents.
						</aside>
					</section>
					<section>
						<h2>Add an index</h2>
						<p>Structured list. Keys point at data so searches are faster.</p>
						<p>Given a term, return id's which contain it.</p>
						<aside class="notes">
The definition of an index can be a little fuzzy, but I am defining it for me as a 
function that I pass in a term or terms and it returns a list of integers
which refer to documents that likely contain those terms.
						</aside>
					</section>
					<section>
						<h2>Textbook 101 index</h2>

<pre><code style="font-size: 14px; line-height: 1em;">
index = map[string][]int{
	"serverless": []int{3,1337},
	"days": []int{1,2,3,4,5,6,1337},
	"brimful": []int{7,45},
	"asha": []int{45},
}
</code></pre>

<p>not compressed, list walking can be slow</p>

						<aside class="notes">
So here is the textbook 101 example.

So serverless days is found in documents 3 and 1337.

Brinful asha is found in 45

This is not compressed and walking those lists for common terms can take a long time.
						</aside>
					</section>
					
					<section>
						<h2>Add Complexity</h2>
						<img src="./img/complex.png" style="border: none;" />
						<aside class="notes">
So we compress the lists to save space using elis fano compression, and
then you have to a data structure called a skip list so you can jump through the lists quickly.

Because we are stuffing the index into a lambda we really want that compression, and we know
lambdas are CPU weak so we want that performance.
						</aside>
					</section>
					<section>
						<h2>My attempt</h2>
						<p>Two turkeys taped together does not make an eagle.</p>
						<img src="./img/turkeys.jpg"  height="500px;" style="border: none;" />
						<aside class="notes">
So I did attempt to create a Compressed SkipList.

And got two turkeys taped together. Not the eagle I wanted.
						</aside>
					</section>
					
				</section>

				<section>
					<section>
						<h2>How about bloom filters?</h2>
						<p>A.K.A Bitsliced signatures</p>
						<aside class="notes">
Now I had the same problem with searchcode.com and had solved it using bloom filters.

There is a technique to search using bloom filters. However it helps if you know what one is.
						</aside>
					</section>
					<section>
						<h2>Bloom filter: Empty</h2>
						<p>16 boolean's in an array</p>
	<img src="./img/BloomFilter1.png" style="border: none;" />
						<aside class="notes">
So what is a bloom filter.
Its a space efficent probablistic data structure.

You add items, and can check if it was added. 

They never return false negatives, but they do occassionaly lie and report something being added when it was not.

So lets look at this example,

We have a 16 bit bloom filter. Its just a slice of booleans set by default to false, indicated by 0 here.
						</aside>
					</section>
					<section>
						<h2>Add</h2>
						<p>Hash the term 3 times and set the bits</p>
	<img src="./img/BloomFilter2.png" style="border: none;" />
						<aside class="notes">
To add something to the filter we get a term, hash it multiple times using a integer returning hash like fnv, 
and use the resulting
outputs to map to bit positions inside our filter. We then set those positions to true.
						</aside>
					</section>
					<section>
						<h2>Add second</h2>
						<p>3 more bits set</p>
	<img src="./img/BloomFilter3.png" style="border: none;" />
						<aside class="notes">
We add another term. Same process, hash 3 times and set the bits.
						</aside>
					</section>
					<section>
						<h2>Overlapping bits</h2>
						<p>"big" and "dog" share 2 bits</p>
	<img src="./img/BloomFilter4.png" style="border: none;" />
						<aside class="notes">
Another term added, where 2 of the bits overlap. This is an expected property of the filter
and one of the ways they are space efficient.
						</aside>
					</section>
					<section>
						<h2>Hit</h2>
						<p>hash "big" and check bits</p>
	<img src="./img/BloomFilter5.png" style="border: none;" />
						<aside class="notes">
To check if the filter has something we do the same process. Hash the term as we did on insert,
then check the bit positions. They are all true so this indicates big was probably added to the filter.
Why probably? We will get to that in a moment.
						</aside>
					</section>
					<section>
						<h2>Miss</h2>
						<p>one bit position is 0 so miss</p>
	<img src="./img/BloomFilter7.png" style="border: none;" />
						<aside class="notes">
Another check, but notice that the middle bit is false. This means we know that sydney was never added
to this filter.
						</aside>
					</section>
					<section>
						<h2>False Positive</h2>
						<p>"big" and "yellow" supplied bits</p>
	<img src="./img/BloomFilter6.png" style="border: none;" />
						<aside class="notes">
Lastly we see an example of a false positive match. We hash and check the positions and they are all true.
But we never added this term, the bits were supplied by others.

You will find that a lot of tweaking of bloom filters is to control the false positive rate.
						</aside>
					</section>


					<section>
						<h2>Bloom filter: search</h2>
						<p>8 bit bloom filter</p>
						<p>Check bit positions 1 and 7. Document 4 matches.</p>
						<pre><code style="font-size: 14px; line-height: 1em;">for each bloomfilter
	for each bit
		check if bit location in filter is set
	if all matching bits are set
		record possible match</code></pre>

						<img src="./img/bloom_naieve_search.gif" height="400px" style="border: none;" />

						<aside class="notes">
So how to search over out bloom filter index? 

You just add more bloom filters.

This represents 4 documents each with their own 8 bit bloom filter. 

So to search we hash out search terms, which come to position 1 and 7. 
We loop over each filter, checking the bit positions at 1 and 7.
If any are false we know it cannot be a match.
If all are true we have a possible match. In this case document 4.
						</aside>
					</section>
				</section>

				<section>
					<section>
						<h2>Advantages</h2>
						<ul>
							<li>Compressed. Only using several bits per term!</li>
							<li>Just arrays of bools, easy to embed in code</li>
						</ul>
						<aside class="notes">
Bloom filters searches have a lot going for them. They are compressed by nature, using a few bits per term added.

Adding is simple, just add a new filter.
Searching is a really simple for loop.
						</aside>
					</section>
					<section>
						<h2>Problem</h2>
						<p>8x overhead per bit because of how languages work...</p>
						<img src="./img/ram.jpg" height="500px" style="border: none;" />
						<aside class="notes">
Firstly there is an 8x overhead when using booleans, because each bool is actually a byte in memory.
This excessabates the second problem.

This search is slow... This comes down to how fetching bits out of 
memory works.

When you probe a single bit on most modern sysems you actually pull back 512 bits from RAM. This is because a 
full cache line is read. So its a 512 times overhead just to test a single bit! 

As a result on average you end up walking ALL the memory for your filter, depending on which bits your probe.

This is one one of the reasons this as an approach fell out of favor in the 70's, because in practice
you do end up walking all the addressable memory.
						</aside>
					</section>
					<section>
						<h2>Illustration</h2>
						<img src="./img/2048_bloom_2.png"  style="border: none;" />
						<aside class="notes">
Here is a representation of the issue. We have a 2048 bit bloom filter, where each square is a bit.
We have hashed our terms to match 6 bit positions.

We used a good hash function so we get a decent distribution within those locations and now want to probe those
bits.
						</aside>
					</section>
					<section>
						<h2>Can we do better?</h2>
						<p> In theory; theory and practice are the same. In practice they arent.</p>
						<img src="./img/2048_bloom_3.png" style="border: none;" />
						<aside class="notes">
Because the bits are all within 512 bits of each other, we are likely to pull the entire filter across 
the CPU.

This is a big problem because accessing memory often isnt as fast as you expect. Modern CPU's can theoretically 
push more than 100 GB across the CPU per second.

In theory; theory and practice are the same. In practice they arent.

Can we do better?
						</aside>
					</section>
				</section>

				
				

				<section>
					<section>
						<h2>Bitfunnel</h2>
						<p>Good enough for Dan Luu? Good enough for you.</p>
						<img src="./img/bitfunnel.png" style="border: none;" />
						<aside class="notes">
In 2017 at SIGIR this paper about bitfunnel was released, detailing how bing uses
bit signatures or bloom filters to power its fresh index.

It won best paper at SIGIR, which is probably the most prestigious IR conferences.

The thing that got my attention is that Dan Luu is a co-author. Anything he writes I pay attention to.
Hence my personal saying "Good enough for Dan Luu, Good enough for you."
						</aside>
					</section>
					<section>
						<h2>Fixes</h2>
						<p>Rotate the filter. Documents now on columns not rows.</p>
						<img src="./img/rotate.gif" height="500px" style="border: none;" />
						<aside class="notes">
In the 80's a smart person called Roberts noticed you could rotate the filter, 
assuming each filter has the same exact size.

So we do that by turning the rows into columns. So each row used to represent a document, 
but now each column does. Filled right to left.
		</aside>
					</section>

					<section>
						<p>Fetch row 1 and 7 same as previous example</p>
						<img src="./img/rotate3.png" height="500px" style="border: none;" />
						<aside class="notes">
With this done we only need to inspect the rows containing the bit positions of the query. 

So like our previous search we are checking bit postitions 1 and 7 which are now rows 1 and 7.
						</aside>
					</section>
					<section>
						<p>Logically & all rows</p>
						<img src="./img/rotate4.png" height="500px" style="border: none;" />
						<aside class="notes">
We then logically & them together and keep the result.

The nice thing about this is that a AND in the CPU is close to being a free operation from a performance 
point of view.
In fact the computation for this is so low it becomes all about the memory lookups.
						</aside>
					</section>
					<section>
						<p>Pos 1 is true, so document 4 matches</p>
						<img src="./img/rotate5.png" height="500px" style="border: none;" />
						<aside class="notes">
So in this particular case, we see a single bit is left on in the first position with 
corresponds to document 4.
This is because we filled right to left, based on our rotation.
						</aside>
					</section>
					<section>
						<h2>Results?</h2>
						<p>This reduces the amount of RAM we need to access by a huge factor for larger bloom filters.</p>
						<aside class="notes">
The result of this is it reduces memory access by a huge amount. Something like 200x.
						</aside>
					</section>

					<section>
						<h2>Example</h2>
						<p>64 bit ints to store the filter</p>
						<p>16 bit bloom filter, with 32 documents added.</p>
						<p>Filled right to left</p>
<pre><code style="font-size: 14px; line-height: 1em;">0000000000000000000000000000000010100011111111111111111111111101
0000000000000000000000000000000001110000000000010000100000000000
0000000000000000000000000000000000000000000000000000000000000000
0000000000000000000000000000000000100011111100000000000000000000
0000000000000000000000000000000000100000000000000000000000000000
0000000000000000000000000000000011000000000000000000000000000000
0000000000000000000000000000000000000000000000000000000000000000
0000000000000000000000000000000000001000000000000001000000000000
0000000000000000000000000000000000111100000000000000000000000000
0000000000000000000000000000000000100001111100000001010101011110
0000000000000000000000000000000010000010000010001010101010100011
0000000000000000000000000000000001011000000000000000000000000000
0000000000000000000000000000000001101011111111111111111111111111
0000000000000000000000000000000000110000001000000001100000001000
0000000000000000000000000000000010100010000111111110101010100001
0000000000000000000000000000000001100011111100000000000000000000
</code></pre>
						<aside class="notes">
This is what it looks like in memory after adding 32 documents. We have a 16 bit bloom filter, 
with 16 int64's one after the other.
We have added 32 documents to it, so only the right most 32 bits have anything set. 

This ensures we are optimally using the space. Because we pack 64 documents into each 
int we end up using a single bit per location in the bloom filter. No 8x overhead!

It also means when we do our & operation we are now searching 64 documents for every operation, which is a huge speed up.
	</aside>
					</section>

					<section>
						<h2>Embed Code Example</h2>
						<p>Example filter.</p>
						<p>50,000 * 2,048 = 102,400,000 ints</p>
<pre><code style="font-size: 14px; line-height: 1em;">
var bloomFilter = []uint64{1942, 1696, 1762, 496, 1776, 1954, 1970, 1536, 494, 134, 128, 1680, 0, 1536, 
2016, 1952, 2047, 296, 1600, 1536, 0, 64, 1664, 1985, 2046, 2032, 1760, 1536, 416, 1536, 360, 1568, 256, 
1920, 384, 0, 0, 1780, 1920, 1536, 256, 2032, 0, 1792, 1536, 1540, 1988, 0, 146, 1664, 2047, 288, 256, 1888, 
256, 2011, 128, 1778, 1904, 354, 0, 200, 1952, 496, 1920, 403, 1687, 384, 128, 1600, 1664, 0, 1600, 1990, 1760, 
1536, 256, 0, 0, 1664, 418, 1860, 1952, 256, 128, 162, 1736, 266, 64, 1922, 64, 1800, 0, 2003, 1920, 2016, 384, 
// snip lots of integers here
1644, 1864, 1920, 64}

</code></pre>
						<aside class="notes">
This is what the filter looks like in code, where it literally is just a huge array of integers.

For 50,000 documents its about 100 million integers.
	</aside>
					</section>

					<section>
						<h2>Searching</h2>
						<p>The core loop.</p>
<pre><code style="font-size: 14px; line-height: 1em;">func Search(queryBits []uint64) []int {
	var results []int
	var res uint64

	for i := 0; i < len(bloomFilter); i += 2048 {
		res = bloomFilter[queryBits[0]+uint64(i)]

		for j := 1; j < len(queryBits); j++ {
			res = res & bloomFilter[queryBits[j]+uint64(i)]

			if res == 0 { // important! skip shard if nothing!
				break
			}
		}

		if res != 0 {
			for j := 0; j < 64; j++ {
				if res&(1<&lt;j) > 0 {
					results = append(results, 64*(i/2048)+j)
				}
			}
		}

	}

	return results
}
</code></pre>
						<aside class="notes">
The core search loop.

I find this algorithm beautifully simple.
				
The result of the above returns a list of interesting document id's which can then be brute force checked
for the terms we believe are in there. We only believe because of the false positive property. 
But the above is fast enough to run in a 
few milliseconds cutting down on the total number of documents we need to inspect to a manageable level.

	</aside>
					</section>
					
					<section>
						<h2>Searching: Visually</h2>
						<p>Perform & between each row, and if we see 0 skip to next block</p>
						<p>~3 ms to run in lambda</p>
						<img src="./img/bloomsearch.gif" height="500px" style="border: none;" /><br>
						<aside class="notes">
Visually the search looks a bit like this. Where it pulls back rows, 
logically anding them and moving to the 
next before jumping to the next block, or skipping ahead where it finds nothing.

Inside a lambda this runs in a few milliseconds.
						</aside>
					</section>
				</section>
					
				<section>
					<section>
						<h2>Crawling</h2>
						<p>Lists of top domains exist, just download 3 or 4 and merge.</p>
						<pre><code style="font-size: 14px; line-height: 1em;">for y in listOfUrls:
	get y</code></pre>
						
						<aside class="notes">
Need to source the websites... thankfully lists of this exist, so I just downloaded them and filtered to anything
ending with .au and merged them.

I tried some crawlers, but just went back to a custom one locked per domain writing out the contents to a custom format.
Dont overthink your crawlers beyond faking the user agent to googlebot.

One advantage about using domain lists is they come with a rank value. Which we can use to get our own pagerank
without spending time implementing it outselves.

I used this value to influence the score giving a “cheap” ranking value.
						</aside>
					</section>
					<section>
						<h2>Output</h2>
<pre><code style="font-size: 14px; line-height: 1em;">
[{
    "url": "https://engineering.kablamo.com.au/",
    "title": [
        "Kablamo Engineering Blog"
    ],
    "h1": [
        "Lessons Learnt Building for the Atlassian Marketplace",
        "What I Wish I Knew About CSS When Starting Out As A Frontender",
        "How to model application flows in React with finite state machines and XState"
    ],
    "h2h3": [
        "Our Partners"
    ],
    "h4h5h6": null,
    "content": [
        "THE BLOG",
        "Insights from the Kablamo Team.",
        "28.7.2021 - By Ben Boyter"
    ],
    "rank": 0.65
}]
</code></pre>
						<aside class="notes">
Here is an example of a crawled document. Note I threw away all of the HTML, which gives something like a 50x saving
on disk storage. It does mean I have to recrawl if I change the index, but you need to crawl freqently anyway so the cost is minimal.

						</aside>
					</section>
					<section>
						<h2>Query Ranking</h2>
						<p>Post query ranking + Pre ranking = happy user</p>
<pre><code style="font-size: 14px; line-height: 1em;">// defaults for BM25 which provide a good level of damping
k1 := 1.2
b := 0.75

for word, wordCount := range res.matchWords {
    freq := documentFrequencies[word]

    tf := float64(wordCount) / words
    idf := math.Log10(float64(corpusCount) / float64(freq))

    step1 := idf * tf * (k1 + 1)
    step2 := tf + k1*(1-b+(b*words/averageDocumentWords))

    weight += step1 / step2
}
</code></pre>
<aside class="notes">
So we have our pre-ranking with our cheap version of page-rank but you also want a query rank that 
reranks based on whatever the user queries on. Otherwise you get really weird results.

Ranking is one of those secret sauces things, but generally TF/IDF or BM25 works fine. 
The above is BM25. Its not that hard to implement. 
</aside>
					</section>
					<section>
						<h2>Snippet Extraction</h2>
						<p>https://github.com/boyter/cs/blob/master/snippet.go</p>
<blockquote>
features, noble mien, and the report which was in general circulation within five minutes after his entrance, of his having ten thousand a year. The gentlemen pronounced him to be a fine
</blockquote>

<blockquote>
it. Dear, dear Lizzy. A house in town! Every thing that is charming! Three daughters married! Ten thousand a year! Oh, Lord! What will become of me. I shall go distracted.”
</blockquote>

<aside class="notes">
Snippet extraction is one of those things that could be a talk of its own.

Also listed as one of the reasons inktomi (anyone remember them?) was beaten by google. Because they didnt have it.

Im not going to walk though it, its fairly complex but the linked code I am particullary proud of.

I based all my string tests on pride and prejudice (which I know more about than I probably should), with the above showing the 2 best matches for "ten thousand a year"

This is eaily the most expensive portion of the search process.
</aside>
					</section>

					<section>
						<h2>Searching</h2>
						<img src="./img/filterdown.png" height="500px" style="border: none;" /><br>
<aside class="notes">
So we now have everything we need to produce a search result.

We start by filtering down using out bloom filter index.
Then brute force search the results to remove the false matches.
The rank the results, and sort them.
Then get the top 20 results and extract the snippets and return.
</aside>
					</section>
					<section>
						<h2>Time to search</h2>
						<p>Problem solved.</p>
<pre><code style="font-size: 14px; line-height: 1em;">2021-09-13T14:33:34.114+10:00	Duration: 142.89 ms Billed Duration: 143 ms
2021-09-13T14:34:26.427+10:00	Duration: 6.44 ms Billed Duration: 7 ms 
2021-09-13T14:35:15.851+10:00	Duration: 3.40 ms Billed Duration: 4 ms 
2021-09-13T14:35:28.738+10:00	Duration: 1.10 ms Billed Duration: 2 ms 
2021-09-13T14:35:44.979+10:00	Duration: 6.11 ms Billed Duration: 7 ms 
2021-09-13T14:36:15.089+10:00	Duration: 70.31 ms Billed Duration: 71 ms 
</code></pre>

<aside class="notes">
With all that done checking cloudwatch shows the following runtime's for a variety of 
searches being 
run, on a lambda allocated with 1024 MB of RAM.

Keep in mind that the above times include ranking and snippet extraction as well and the result is ready to show to the user. 
Its not just the time for the core search.

This is easily good enough to build our engine on.
						</aside>
					</section>
				</section>


				<section>
					<section>
						<h2>Design</h2>
						<img src="./img/design.png" height="500px" style="border: none;" /><br>
						<aside class="notes">
API Gateway fronts a single controller lambda which calls multiple workers as configured in its environment variables. 
The workers contain the index, and when passed a query they search over their content finding matches, ranking them 
and then returning the top 20 results. The controller waits for all the results to return, joins them together, 
re-sorts and sends the top results back.

Lastly I call the API endpoint with a small http server and mark up the results.
						</aside>
					</section>
					<section>
						<h2>Building Index</h2>
						<p>Iterate the crawled documents</p>
						<pre><code style="font-size: 14px; line-height: 1em;">
sb.WriteString(fmt.Sprintf(`var averageDocumentLength float64 = %d`, averageDocumentLength))

sb.WriteString(`var documentFrequencies = map[string]uint32{`)
for k, v := range newFreq {
    sb.WriteString(fmt.Sprintf("\"%s\": %d,", k, v))
}
sb.WriteString("}")

sb.WriteString("var bloomFilter = []uint64{")
for _, v := range bloom {
    sb.WriteString(fmt.Sprintf("%d,", v))
}
sb.WriteString("}")

_, _ = file.WriteString(fmt.Sprintf(`{Url:"%s",Title:"%s",Content:"%s",Score:%.4f},`,
			res.Url,
			res.Title,
			res.Content,
			res.Score))
</code></pre>
						<p>Write out to content.go.1 content.go.2 etc...</p>
<aside class="notes">
Now we have to build the index. I wrote a script to iterate all the crawled documents
trying to stuff them together into batches that would create a 50 mb zipped binary file.

When done it writes out index, the frequencies and the document as Go code written to files
we can the compile into the application.
</aside>
					</section>


					<section>
						<h2>Deployment</h2>

<pre><code style="font-size: 14px; line-height: 1em;">		
for i in {1..1000}
do
  cp ./content.go."$i" ./content.go
  GOOS=linux GOARCH=amd64 go build -ldflags="-s -w" -o main && 
  	zip main.zip main && 
	aws lambda create-function --function-name t"$i" --runtime go1.x --zip-file fileb://main.zip \
		--handler main --timeout 10 --memory-size 1024 \
		--role arn:aws:iam::000000000000:role/aws-lambda-search-LambdaServiceRole-1NOLIFEIFREAD
done

aws lambda update-function-configuration --function-name aws-lambda-search-controller \
	--environment '{"Variables": {"WORKERS": "t0,t1,t2,t3,t4,t5,t6,t7,t8,t9,t10,t11,t12,t13,t14,t15,t16,t17,t18,t19,t20,t21,t22,t23,t24,t25,t26,t27,t28,t29,t30,t31,t32,t33,t34,t35,t36,t37,t38,t39,t40,t41,t42,t43,t44,t45,t46,t47,t48,t49,t50,t51,t52,t53,t54,t55,t56,t57,t58,t59,t60,t61,t62,t63,t64,t65,t66,t67,t68,t69,t70,t71,t72,t73,t74,t75,t76,t77,t78,t79,t80,t81,t82,t83,t84,t85,t86,t87,t88,t89,t90,t91,t92,t93,t94,t95,t96,t97,t98,t99,t100,t101,t102,t103,t104,t105,t106,t107,t108,t109,t110,t111,t112,t113,t114,t115,t116,t117,t118,t119,t120,t121,t122,t123,t124,t125,t126,t127,t128,t129,t130,t131,t132,t133,t134,t135,t136,t137,t138,t139,t140,t141,t142,t143,t144,t145,t146,t147,t148,t149,t150,t151,t152,t153,t154,t155,t156,t157,t158,t159,t160,t161,t162,t163,t164,t165,t166,t167,t168,t169,t170,t171,t172,t173,t174,t175,t176,t177,t178,t179,t180,t181,t182,t183,t184,t185,t186,t187,t188,t189,t190,t191,t192,t193,t194,t195,t196,t197,t198,t199,t200,t201,t202,t203,t204,t205,t206,t207,t208,t209,t210,t211,t212,t213,t214,t215,t216,t217,t218,t219,t220,t221,t222,t223,t224,t225,t226,t227,t228,t229,t230,t231,t232,t233,t234,t235,t236,t237,t238,t239,t240,t241,t242,t243,t244,t245,t246,t247,t248,t249,t250,t251,t252,t253,t254,t255,t256,t257,t258,t259,t260,t261,t262,t263,t264,t265,t266,t267,t268,t269,t270,t271,t272,t273,t274,t275,t276,t277,t278,t279,t280,t281,t282,t283,t284,t285,t286,t287,t288,t289,t290,t291,t292,t293,t294,t295,t296,t297,t298,t299,t300,t301,t302,t303,t304,t305,t306,t307,t308,t309,t310,t311,t312,t313,t314,t315,t316,t317,t318,t319,t320,t321,t322,t323,t324,t325,t326,t327,t328,t329,t330,t331,t332,t333,t334,t335,t336,t337,t338,t339,t340,t341,t342,t343,t344,t345,t346,t347,t348,t349,t350,t351,t352,t353,t354,t355,t356,t357,t358,t359,t360,t361,t362,t363,t364,t365,t366,t367,t368,t369,t370,t371,t372,t373,t374,t375,t376,t377,t378,t379,t380,t381,t382,t383,t384,t385,t386,t387,t388,t389,t390,t391,t392,t393,t394,t395,t396,t397,t398,t399,t400,t401,t402,t403,t404,t405,t406,t407,t408,t409,t410,t411,t412,t413,t414,t415,t416,t417,t418,t419,t420,t421,t422,t423,t424,t425,t426,t427,t428,t429,t430,t431,t432,t433,t434,t435,t436,t437,t438,t439,t440,t441,t442,t443,t444,t445,t446,t447,t448,t449,t450,t451,t452,t453,t454,t455,t456,t457,t458,t459,t460,t461,t462,t463,t464,t465,t466,t467,t468,t469,t470,t471,t472,t473,t474,t475,t476,t477,t478,t479,t480,t481,t482,t483,t484,t485,t486,t487,t488,t489,t490,t491,t492,t493,t494,t495,t496,t497,t498,t499,t500,t501,t502,t503,t504,t505,t506,t507,t508,t509,t510,t511,t512,t513,t514,t515,t516,t517,t518,t519,t520,t521,t522,t523,t524,t525,t526,t527,t528,t529,t530,t531,t532,t533,t534,t535,t536,t537,t538,t539,t540,t541,t542,t543,t544,t545,t546,t547,t548,t549,t550,t551,t552,t553,t554,t555,t556,t557,t558,t559,t560,t561,t562,t563,t564,t565,t566,t567,t568,t569,t570,t571,t572,t573,t574,t575,t576,t577,t578,t579,t580,t581,t582,t583,t584,t585,t586,t587,t588,t589,t590,t591,t592,t593,t594,t595,t596,t597,t598,t599,t600,t601,t602,t603,t604,t605,t606,t607,t608,t609,t610,t611,t612,t613,t614,t615,t616,t617,t618,t619,t620,t621,t622,t623,t624,t625,t626,t627,t628,t629,t630,t631,t632,t633,t634,t635,t636,t637,t638,t639,t640,t641,t642,t643,t644,t645,t646,t647,t648,t649,t650,t651,t652,t653,t654,t655,t656,t657,t658,t659,t660,t661,t662,t663,t664,t665,t666,t667,t668,t669,t670,t671,t672,t673,t674,t675,t676,t677,t678,t679,t680,t681,t682,t683,t684,t685,t686,t687,t688,t689,t690,t691,t692,t693,t694,t695,t696,t697,t698,t699,t700,t701,t702,t703,t704,t705,t706,t707,t708,t709,t710,t711,t712,t713,t714,t715,t716,t717,t718,t719,t720,t721,t722,t723,t724,t725,t726,t727,t728,t729,t730,t731,t732,t733,t734,t735,t736,t737,t738,t739,t740,t741,t742,t743,t744,t745,t746,t747,t748,t749,t750,t751,t752,t753,t754,t755,t756,t757,t758,t759,t760,t761,t762,t763,t764,t765,t766,t767,t768,t769,t770,t771,t772,t773,t774,t775,t776,t777,t778,t779,t780,t781,t782,t783,t784,t785,t786,t787,t788,t789,t790,t791,t792,t793,t794,t795,t796,t797,t798,t799,t800"}}'
</code></pre>

<img src="./img/deployment.png" height="300px" style="border: none;" />

<aside class="notes">
Deployment is a simple matter of iterating over each of the content files we wrote out previously
Then building the binary, zipping it and create it using the aws command line tools.

I used the command line tools because creating 1000 lambdas in a single CF was painful
I can quickly update them this way and I dont need to use S3 to store the binary at any point.

Lastly we update the controller lambda to know tne names of workers and we are done.
</aside>
					</section>
				</section>

				<section>
					<section>
						<h2>Results...</h2>
						<ul>
							<li>~50 million documents indexed</li>
							<li>~1000 lambda functions</li>
							<li>23.6 GB (31% of 75 GB) FREE STORAGE!!!!</li>
							<li>In fact the whole thing is FREE if not being called!</li>
							<li>TRUE SERVERLESS SEARCH!</li>
						</ul>
					</section>
					<section>
						<h2>bonzamate.com.au</h2>
						<img src="./img/search1.png" height="500px" style="border: none;" /><br>
<aside class="notes">
So here is the UI. To make it look a little more real I added a news
feed again fully serverless.
</aside>
					</section>
					<section>
						<img src="./img/search2.png" height="500px" style="border: none;" /><br>
<aside class="notes">
The results of a search, with the relevant news on the right and results as expected.
</aside>
					</section>
					<section>
						<img src="./img/search3.png" height="500px" style="border: none;" /><br>
<aside class="notes">
It was still feeling empty so I parsed a wikipedia dump for Aussie content and added
another serverless function with zeroclick information in it that is displayed if there is a match.

In this case a search for HMAS Onslow shows the relevant wikipedia info on the right.
</aside>
					</section>
					<section>
						<img src="./img/search4.png" height="500px" style="border: none;" /><br>
<aside class="notes">
Lastly a working vanity search for myself, because hey its my search engine.
</aside>
					</section>
					<section>
						<p>What is the sound of 1000 lambdas searching themselves?</p>
						<img src="./img/cloudwatch.png" height="500px" style="border: none;" /><br>
<aside class="notes">
Probably something like cha-ching once the free credits run out. 

The results in cloudwatch over the last 6 months or so.
</aside>
					</section>
				</section>

				<section>
					<section>
						<h2>Conclusions</h2>
						<p>Go. I doubt I could have done it in another language.</p>
						<p>Bloom filters are great!</p>
						<p>AWS is ripe for abuse like this</p>
						<p>AMD64 vs ARM64</p>
						<aside class="notes">
	So conclusions.
	I doubt I could have done this in another language. 
	Remember we compile the index as code, so we want a fast compiler, which Go has.
	Java has the class file limits, Rust takes too long to compile, Python probably too slow.

	Bloom filters are great. Such a simple data structure. 
	People suggest things like xor or cuckuoo filters over them, but their simplicity 
	makes them worth knowing about.

	AWS is ripe for abuse, and you should.

	Someone once asked me about ARM64. 
	I did try Graviton lambdas, but the binaries were larger, so I needed more lambdas, 
	so little to gain. 
						</aside>
					</section>
					<section>
						<h2>Conclusions</h2>
						<ul>
							<li>Lambda layers... pointless for this, CPU weak</li>
							<li>Its... probably not a good idea at this scale</li>
							<li>Probably a really good idea for static websites?</li>
						</ul>
						<p>A minimal version of the bloom filter index is available for you to play with <a href="https://github.com/boyter/indexer">https://github.com/boyter/indexer</a></p>
						<aside class="notes">
I was also asked about lambda layers, but the CPU is probably too weak to really use it,
and besides then I would be paying for storage.

Its probably not a good idea to do things like this at scale, 
as the cost can be quite high if it gets popular. I actaully had to take some very 
aggressive measures against bots to avoid some large bills.

This might be a really good idea for static websites, especially with lambda urls?

So I have a version of the bloom filter index for you to play with if interested 
in doing something like this yourself.
				</aside>
					</section>
				</section>

				<section>
					<img src="./img/vendor_lockin_compressed.png" height="500px" style="border: none;" /><br>
					<aside class="notes">
So thank you for today. I leave you with this comic, partly inspired by this experiment.

I love the final quote.

"Vendor lock in? They wish. All these vendors are locked in here with ME!"
					</aside>
				</section>

			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
