<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Abusing Go, AWS Lambda and bloom filters to make a true Australian serverless search engine</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<h1>Abusing Go, AWS Lambda and bloom filters to make a true Australian serverless search engine</h1>
					<aside class="notes">
Good morning Sydney!
Lovely to see everyone back for day two.
Hope you are all caffinated and comfortable.
As you can mostly see im talking about bloom filers and search. Sorry about the clickbait title, but you gotta stand out in the 
paper selection process somehow.
					</aside>
				</section>

				<section>
					<section>
						<h2>Who are you?</h2>
						<p>
						"Officially" technical lead/principle at Kablamo but a "code monkey" at heart.</p>
						<p>I blog <a href="https://boyter.org/">boyter.org</a> I free software <a href="https://github/boyter/">github/boyter/</a> I run <a href="https://searchcode.com/">searchcode.com</a> also on the twitter.com <a href="https://twitter.com/boyter">@boyter</a> activitypub <a href="https://honk.boyter.org/boyter">@boyter@honk.boyter.org</a></p>
						<aside class="notes">
So who am I? 
I am not going to bore you too much with who I am because you probably don't care. 
Im not famous, but you can find me at these places.
						</aside>
					</section>
					<section>
						<h2>https://boyter.org/</h2>
						<p>Everything is <a href="https://boyter.org/">here</a>, so feel free to go back to sleep</p>
						<img src="./img/halflisten.jpg" height="500px;" style="border: none;" />
						<aside class="notes">
For those who prefer to hang out on slack everything you are about to see is now available boyter.org
Don't worry too much about missing anything.
You can even skip ahead on the slides and head off for a coffee if you like. I won't judge you.
						</aside>
					</section>
				</section>

				<section>
					<section>
						<h2>My hobby; search engines</h2>
							<p>early termination, syntax highlighting, matching, snippet extraction, 
	rate limiting, index, bot detection, ranking, distributed algorithms, caching, tokenization,
	string searching, regular expressions, data structures, line detection, CPU cache lines, duplicate detection, 
	literal extraction, unicode, case insensitive matching etc...</p>
	<aside class="notes">
	Here are some of the things that go into any search engine.

	Every part you see is one of those problems you lightly scratch and discover
	people getting PhD's in. So it constantly keeps me interested and learning.
						</aside>
					</section>
					<section>
						<h2>searchcode.com</h2>
						<img src="./img/searchcode.gif" height="500px;" style="border: none;" />
						<aside class="notes">
So at the time I was building a new index for searchcode from scratch. 
No real reason beyond I find it interesting. I mentioned this to a work colleague and he 
asked why I didn't use AWS as generally for work everything lands there. 
I mentioned something to the effect that you needed a lot of persistent storage, 
or RAM to keep the index around which is prohibitively expensive. 

He mentioned perhaps using Lambda? And I responded its lack of persistance is a problem.
						</aside>
					</section>

					<section>
						<h2>Lambda/Serverless Weaknesses</h2>
						<ul>
							<li>Persistance</li>
							<li>Performance</li>
						</ul>
						<aside class="notes">
Lamba's or any other serverless function on cloud work well for certain problems. 
So long as you can rebuild state inside the lambda, because there is no guarantee it 
will still be running next time the lambda executes. The lack of persistance is an issue 
because modern search engines need to have some level of it. 
You either store the index in RAM, as most modern search engines do, or on disk.

At that point I trailed off. Something occurred to me.

Without persistance it makes lambda a non starter. But, there is a saying in computing.
						</aside>
					</section>

					<section>
						<blockquote>Never do at runtime what you can do at compile time.</blockquote>
						<aside class="notes">
I decided to see how far I can take that idea, by using AWS Lambda to build a search engine. 
How do we get get around the lack of persistance? By baking the index into the lambda themselves. 
In other words, generate code which contains the index and compile that into the lambda binary. 
Do it at compile time.
						</aside>
					</section>
				</section>



				<section>
					<section>
						<h2>Proving the theory</h2>
						<pre><code style="font-size: 14px; line-height: 1em;">package main

func main() {
	index := []string{
		"Memcached vs Redis - More Different Than You Would Expect",
		"You Don't Need a Library for File Walking in Go",
		// ... 99,997 more ...
		"Lessons Learnt Building for the Atlassian Marketplace",
	}

	for _, x := range index {
		strings.Contains(x, "searchterm")
	}
}
</code></pre>
						
						<aside class="notes">
So the first thing I considered was putting content directly into lambda’s, and then brute force 
searching across that content. Considering our guess of storing ~100,000 items in a lambda, a modern 
CPU brute force string searching in memory should return in a few hundred milliseconds. Modern CPU’s are very fast.

So I tried it. I created a Go file with 100,000 strings in a slice, and then wrote a simple loop 
to run over that performing a search.

Alas I underestimated how weak the CPU allotted to a lambda is, and searches took several seconds. 
Even increasing the RAM to improve the CPU allotment didn't really help. My fallback plan was to
 embed an index into the lambda, allowing for a quick scan over that index before looking at the content directly.
						</aside>
					</section>
					<section>
						<h2>This does not work...</h2>
						<img src="./img/lambdacpu.png" style="border: none;" />
						<aside class="notes">
Alas I underestimated how weak the CPU allotted to a lambda is, and searches took several seconds. 
Even increasing the RAM to improve the CPU allotment didn’t really help. 
My fallback plan was to embed an index into the lambda, allowing for a quick scan over that index 
before looking at the content directly.
						</aside>
					</section>
					<section>
						<h2>What is an index?</h2>
						<p>Structured list. Keys point at data so searches are faster.</p>
						<p>Given a term, return id's which contain it.</p>
						<aside class="notes">
So what is an index. The definition can be a little fuzzy, but I am defining it for me as a 
function that I pass in a term or terms and it returns a list of integers
which refer to documents that likely contain those terms, possibly ordered by relevance.
						</aside>
					</section>
					<section>
						<h2>Textbook 101 index</h2>

<pre><code style="font-size: 14px; line-height: 1em;">package main

func main() {
	index := map[string][]int{} 
	index["serverless"] = append(index["serverless"], 1337)
}
</code></pre>

						<aside class="notes">
So here is the textbook 101 example.
Given a term in this case "serverless" lets index it. 

The first index here is a non positional index. Its just a map of string to slice of integers.
Strings are the terms and integers the document ids.

The document id's slice are known as a posting list.
So "gopher" is found inside document 1337 here.

The next example is a positional index.
A positional index is one that stores where the term was found in the document. 

So "gopher" in document 1337 was found in positions 1, 2, 3 and 99.

Positional indexes are really useful for phrase searches and ranking.
One catch with this is that your index tends to becomes as large or larger than the thing you are indexing.
						</aside>
					</section>
					
					<section>
						<h2>Complexity</h2>
						<img src="./img/complex.png" style="border: none;" />
						<aside class="notes">
This introduces complexity.
This is clearly a lot more complex than what we started with. 
We need to build a CompressedSkipList.
						</aside>
					</section>
					<section>
						<h2>My attempt</h2>
						<p>Two turkeys taped together does not make an eagle.</p>
						<img src="./img/turkeys.jpg"  height="500px;" style="border: none;" />
						<aside class="notes">
This is what my attempted looked like.

So I did attempt to create a CompressedSkipList.

I implemented both the skip list and elias fano compression, and got two turkeys taped together.
Not the eagle I wanted.
						</aside>
					</section>
					
				</section>

				<section>
					<section>
						<h2>How about bloom filters?</h2>
						<p>A.K.A Bitsliced signatures</p>
						<aside class="notes">
There is a technique to search using bloom filters. However it helps if you know what one is.
						</aside>
					</section>
					<section>
						<h2>Bloom filter: Empty</h2>
						<p>16 boolean's in an array</p>
	<img src="./img/BloomFilter1.png" style="border: none;" />
						<aside class="notes">
So what is a bloom filter.
In short a space efficent probablistic data structure.

You add items, and can check if it was added. 

They never return false negatives, but they do occassionaly lie and report something being added when it was not.

So lets look at this example,

We have a 16 bit bloom filter. Its just a slice of booleans set by default to false, indicated by 0 here.
						</aside>
					</section>
					<section>
						<h2>Bloom filter: Add</h2>
						<p>Hash the term 3 times and set the bits</p>
	<img src="./img/BloomFilter2.png" style="border: none;" />
						<aside class="notes">
To add something to the filter we get a term, hash it multiple times using a integer returning hash like fnv, 
and use the resulting
outputs to map to bit positions inside our filter. We then set those positions to true.
						</aside>
					</section>
					<section>
						<h2>Bloom filter: Add second</h2>
						<p>3 more bits set</p>
	<img src="./img/BloomFilter3.png" style="border: none;" />
						<aside class="notes">
We add another term. Same process, hash 3 times and set the bits.
						</aside>
					</section>
					<section>
						<h2>Bloom filter: Add overlapping bits</h2>
						<p>"big" and "dog" share 2 bits</p>
	<img src="./img/BloomFilter4.png" style="border: none;" />
						<aside class="notes">
Another term added, where 2 of the bits overlap. This is an expected property of the filter
and one of the ways they are space efficient.
						</aside>
					</section>
					<section>
						<h2>Bloom filter: Hit</h2>
						<p>hash "big" and check bits</p>
	<img src="./img/BloomFilter5.png" style="border: none;" />
						<aside class="notes">
To check if the filter has something we do the same process. Hash the term as we did on insert,
then check the bit positions. They are all true so this indicates big was probably added to the filter.
Why probably? We will get to that in a moment.
						</aside>
					</section>
					<section>
						<h2>Bloom filter: Miss</h2>
						<p>one bit position is 0 so miss</p>
	<img src="./img/BloomFilter7.png" style="border: none;" />
						<aside class="notes">
Another check, but notice that the middle bit is false. This means we know that sydney was never added
to this filter.
						</aside>
					</section>
					<section>
						<h2>Bloom filter: False Positive</h2>
						<p>"big" and "yellow" supplied bits</p>
	<img src="./img/BloomFilter6.png" style="border: none;" />
						<aside class="notes">
Lastly we see an example of a false positive match. We hash and check the positions and they are all true.
But we never added this term, the bits were supplied by other terms.

Now while bloom filters do introduce false positives, remember we have to deal with them anyway 
since trigrams produce the same problem.

You will find that a lot of tweaking of bloom filters is to control the false positive rate.
						</aside>
					</section>

					<!-- <section>
						<h2>Go Bloom Filter</h2>

<pre><code style="font-size: 14px; line-height: 1em;">package main

import (
	"fmt"
	"hash/fnv"
)

func main() {
	bloom := make([]bool, 16)          // 16 bit bloom filter
	hash := func(term string) uint64 { // single hash function for bloom filter
		hsh := fnv.New64()
		_, _ = hsh.Write([]byte(term))
		return hsh.Sum64() % uint64(len(bloom))
	}

	bloom[hash("gopher")] = true // add to the filter
	bloom[hash("sydney")] = true

	for _, i := range bloom { // print out the filter
		if i == true {
			fmt.Print("1")
		} else {
			fmt.Print("0")
		}
	}

	if bloom[hash("sydney")] == true { fmt.Print("\nprobably added") }
	if bloom[hash("house")] == false { fmt.Print("\nwas not added") }
	if bloom[hash("boyter")] == true { fmt.Print("\nfalse positive! was never added!") }
}
</code></pre>

<pre><code style="font-size: 14px; line-height: 1em;">$ go run main.go 
0010000001000000
probably added
was not added
false positive! was never added!
</code></pre>

						<aside class="notes">
So here is a bloom filter implemented in go. The filter itself is 4 lines of code. A slice of booleans,
a function to hash and thats all you need. The output shows that terms were added, and a false positive.
						</aside>
					</section> -->

					<section>
						<h2>Bloom filter: search</h2>
					
						<p>Check bit positions 1 and 7. Document 4 matches.</p>
						<pre><code style="font-size: 14px; line-height: 1em;">for each bloomfilter
	for each bit
		check if bit location in filter is set
	if all matching bits are set
		record possible match</code></pre>

						<img src="./img/bloom_naieve_search.gif" height="400px" style="border: none;" />

						<aside class="notes">
So how to search over out bloom filter index? 

You just add more bloom filters.

This represents 4 documents each with their own 8 bit bloom filter. 

So to search we hash out search terms, which come to position 1 and 7. 
We loop over each filter, checking the bit positions at 1 and 7.
If any are false we know it cannot be a match.
If all are true we have a possible match. In this case document 4.
						</aside>
					</section>
				</section>

				<section>
					<section>
						<h2>Advantages</h2>
						<ul>
							<li>Compressed. Only using several bits per term!</li>
							<li>Just arrays of integers, easy to embed</li>
						</ul>
						<aside class="notes">
Bloom filters searches have a lot going for them. They are compressed by nature, using a few bits per term added.
They are also really simple to implement as you just saw, its only 5 lines of code.
Adding is simple, just add a new filter.
Editing is simple, just fiddle some values or replace the filter.
Searching is a really simple for loop.
You can extend them to counting bloom filters, or put structs for positions.

This technique lends itself pretty well to what I am attempting to do, because ultimately it’s just an 
array of 64 bit integers you scan across, making it trivial to write this out into a file which you then compile. Its also already compressed ensuring we can stay under our 50 MB limit while storing a lot of content. Lastly the actual code to do the search is a simple loop with some bitwise checks. Far easier to deal with than a skiplist, which would need to be written into code.
						</aside>
					</section>
					<section>
						<h2>Problems</h2>
						<p>No free lunch...</p>
						<p>8x overhead per bit because of how languages work...</p>
						<aside class="notes">
There are of course problems.

Firstly.

How much memory does 100 million bools take up? Almost 100 MB which if you calculate is about 8x what you expect.
						</aside>
					</section>
					<section>
						<h2>Problem 2</h2>
						<p>Generic RAM stick</p>
						<img src="./img/ram.jpg" height="500px" style="border: none;" />
						<aside class="notes">
Another problem is that is that this search is slow... This comes down to how fetching bits out of 
memory works.

When you probe a single bit on most modern sysems you actually pull back 512 bits from RAM. This is because a 
full cache line is read. So its a 512 times overhead just to test a single bit! 

As a result on average you end up walking ALL the memory for your filter, depending on which bits your probe.

This is one one of the reasons this as an approach fell out of favor in the 70's, because in practice
you do end up walking all the addressable memory.
						</aside>
					</section>
					<section>
						<h2>Illustration</h2>
						<img src="./img/2048_bloom_2.png"  style="border: none;" />
						<aside class="notes">
Here is a representation of the issue. We have a 2048 bit bloom filter, where each square is a bit.
We have hashed our terms to match 6 bit positions.

We used a good hash function so we get a decent distribution within those locations and now want to probe those
bits.
						</aside>
					</section>
					<section>
						<h2>Can we do better?</h2>
						<img src="./img/2048_bloom_3.png" style="border: none;" />
						<aside class="notes">
Because the bits are all within 512 bits of each other, we are likely to pull the entire filter across 
the CPU.

This is a big problem because accessing memory often isnt as fast as you expect. Modern CPU's can theoretically 
push more than 100 GB across the CPU per second.

In theory theory and practice are the same. In practice they arent.

In reality you are more likely to get 50 GB. So if your index is 100 GB in size
you are limited to 1 search every two seconds. 

Can we do better?
						</aside>
					</section>
				</section>

				
				

				<section>
					<section>
						<h2>Bitfunnel</h2>
						<p>Good enough for Dan Luu? Good enough for you.</p>
						<img src="./img/bitfunnel.png" style="border: none;" />
						<aside class="notes">
In 2017 at SIGIR this paper about bitfunnel was released, detailing how bing uses
bit signatures or bloom filters to power its fresh index.

It won best paper at SIGIR, which is probably the most prestigious IR conferences.

The thing that got my attention is that Dan Luu is a co-author. Anything he writes I pay attention to.
Hence my personal saying "Good enough for Dan Luu, Good enough for you."
						</aside>
					</section>
					<section>
							<h2>Fixes</h2>
							<p>Rotate the filter. Documents now on columns not rows.</p>
							<img src="./img/rotate.gif" height="500px" style="border: none;" />
							<aside class="notes">
In the 80's a smart person called Roberts noticed you could rotate the filter, 
assuming each filter has the same exact size.

So we do that by turning the rows into columns. So each row used to represent a document, 
but now each column does. Filled right to left.
		</aside>
					</section>

					<section>
						<p>Fetch row 1 and 7 same as previous example</p>
						<img src="./img/rotate3.png" height="500px" style="border: none;" />
						<aside class="notes">
With this done we only need to inspect the rows containing the bit positions of the query. 

So like our previous search we are checking bit postitions 1 and 7 which are now rows 1 and 7.
						</aside>
					</section>
					<section>
						<p>Logically & all rows</p>
						<img src="./img/rotate4.png" height="500px" style="border: none;" />
						<aside class="notes">
We then logically & them together and keep the result.

The nice thing about this is that a AND in the CPU is close to being a free operation from a performance 
point of view.
In fact the computation for this is so low it becomes all about the memory lookups.
						</aside>
					</section>
					<section>
						<p>Pos 1 is true, so document 4 matches</p>
						<img src="./img/rotate5.png" height="500px" style="border: none;" />
						<aside class="notes">
So in this particular case, we see a single bit is left on in the first position with 
corresponds to document 4.
This is because we filled right to left, based on our rotation.
						</aside>
					</section>
					<section>
						<h2>Results?</h2>
						<p>This reduces the amount of RAM we need to access by a huge factor for larger bloom filters.</p>
						<aside class="notes">
The result of this is it reduces memory access by a huge amount. Something like 200x.
						</aside>
					</section>

					<section>
						<h2>Result</h2>
						<p>16 bit bloom filter, with 32 documents added. Less wasted space.</p>
<pre><code style="font-size: 14px; line-height: 1em;">0000000000000000000000000000000010100011111111111111111111111101
0000000000000000000000000000000001110000000000010000100000000000
0000000000000000000000000000000000000000000000000000000000000000
0000000000000000000000000000000000100011111100000000000000000000
0000000000000000000000000000000000100000000000000000000000000000
0000000000000000000000000000000011000000000000000000000000000000
0000000000000000000000000000000000000000000000000000000000000000
0000000000000000000000000000000000001000000000000001000000000000
0000000000000000000000000000000000111100000000000000000000000000
0000000000000000000000000000000000100001111100000001010101011110
0000000000000000000000000000000010000010000010001010101010100011
0000000000000000000000000000000001011000000000000000000000000000
0000000000000000000000000000000001101011111111111111111111111111
0000000000000000000000000000000000110000001000000001100000001000
0000000000000000000000000000000010100010000111111110101010100001
0000000000000000000000000000000001100011111100000000000000000000
</code></pre>
						<aside class="notes">
This is what it looks like in memory after adding 32 documents. We have a 16 bit bloom filter, 
with 16 int64's one after the other.
We have added 32 documents to it, so only the right most 32 bits have anything set. 

This ensures we are optimally using the space. Because we pack 64 documents into each 
int we end up using a single bit per location in the bloom filter. No 8x overhead!

It also becomes very fast to iterate this in code because its a simple for loop over a slice. 

It also means when we do our & operation we are now searching 64 documents for every operation, which is a huge speed up.
	</aside>
					</section>

					<section>
						<h2>Code Example</h2>
						<p>Example filter.</p>
<pre><code style="font-size: 14px; line-height: 1em;">
var bloomFilter = []uint64{1942, 1696, 1762, 496, 1776, 1954, 1970, 1536, 494, 134, 128, 1680, 0, 1536, 
2016, 1952, 2047, 296, 1600, 1536, 0, 64, 1664, 1985, 2046, 2032, 1760, 1536, 416, 1536, 360, 1568, 256, 
1920, 384, 0, 0, 1780, 1920, 1536, 256, 2032, 0, 1792, 1536, 1540, 1988, 0, 146, 1664, 2047, 288, 256, 1888, 
256, 2011, 128, 1778, 1904, 354, 0, 200, 1952, 496, 1920, 403, 1687, 384, 128, 1600, 1664, 0, 1600, 1990, 1760, 
1536, 256, 0, 0, 1664, 418, 1860, 1952, 256, 128, 162, 1736, 266, 64, 1922, 64, 1800, 0, 2003, 1920, 2016, 384, 
// snip lots of integers here
1644, 1864, 1920, 64}

</code></pre>
						<aside class="notes">
This is what the filter looks like in code, where it literally is just a huge array of integers. I used 2048
integers per 64 documents, in my case to make things simple.
	</aside>
					</section>

					<section>
						<h2>Searching</h2>
						<p>The core loop.</p>
<pre><code style="font-size: 14px; line-height: 1em;">func Search(queryBits []uint64) []int {
	var results []int
	var res uint64

	for i := 0; i < len(bloomFilter); i += 2048 {
		res = bloomFilter[queryBits[0]+uint64(i)]

		for j := 1; j < len(queryBits); j++ {
			res = res & bloomFilter[queryBits[j]+uint64(i)]

			if res == 0 { // important! skip shard if nothing!
				break
			}
		}

		if res != 0 {
			for j := 0; j < 64; j++ {
				if res&(1<&lt;j) > 0 {
					results = append(results, 64*(i/2048)+j)
				}
			}
		}

	}

	return results
}
</code></pre>
						<aside class="notes">
The core search loop.

I find this algorithm beautifully simple.
				
The result of the above returns a list of interesting document id’s which can then be brute force checked
for the terms we believe are in there. We only believe because of the false positive property that bloom filters have. 
Bloom filters by default produce false positive results, but the above is fast enough to run in a 
few milliseconds cutting down on the total number of documents we need to inspect to a manageable level.

	</aside>
					</section>
					
					<section>
						<h2>Searching: Visually</h2>
						<p>Perform & between each row, and if we see 0 skip to next block</p>
						<img src="./img/bloomsearch.gif" height="500px" style="border: none;" /><br>
						<aside class="notes">
Visually the search looks a bit like this. Where it pulls back rows, 
logically anding them and moving to the 
next before jumping to the next block, or skipping ahead where it finds nothing.
						</aside>
					</section>

					<section>
						<h2>Time to search</h2>
<pre><code style="font-size: 14px; line-height: 1em;">
2021-09-13T14:33:34.114+10:00	Duration: 142.89 ms Billed Duration: 143 ms
2021-09-13T14:34:26.427+10:00	Duration: 6.44 ms Billed Duration: 7 ms 
2021-09-13T14:35:15.851+10:00	Duration: 3.40 ms Billed Duration: 4 ms 
2021-09-13T14:35:28.738+10:00	Duration: 1.10 ms Billed Duration: 2 ms 
2021-09-13T14:35:44.979+10:00	Duration: 6.11 ms Billed Duration: 7 ms 
2021-09-13T14:36:15.089+10:00	Duration: 70.31 ms Billed Duration: 71 ms 
</code></pre>

<aside class="notes">
Checking cloudwatch with the above implemented shows the following runtime’s for a variety of searches being 
run, on a lambda allocated with 1024 MB of RAM.

Keep in mind that the above times include ranking and snippet extraction as well and the result is ready to show to the user. 
Its not just the time for the core search.						
						
						</aside>
					</section>
				</section>
					
				<section>
					<section>
						<h2></h2>
						<p>https://hackertarget.com/top-million-site-list-download/</p>
						<pre><code style="font-size: 14px; line-height: 1em;">
Of course everyone knows that PageRank by Google is what propelled Google to the top of the search heap… 
Actually I don’t know how true that is and I suspect that speed and not being a portal helped, but regardless, 
pagerank requires processing your documents multiple times to produce the rank score for the domain or page. 
It takes a long time to do this, and while the whole thing beautiful mathematically, its not very practical
especially for a single person working on this in their spare time.

Can we find an easier way to do this? Some shortcut? Well yes. Turns out that all of the document sources where 
I got the domains, list those domains in order of popularity. So I used this value to influence the score giving a 
“cheap” version of pagerank. Adding the domain popularity into the index when building it provides some pre-ranking of documents.
						</code></pre>
					</section>
					<section>
						<h2>Crawling...</h2>
<pre><code style="font-size: 14px; line-height: 1em;">
{
    "url": "https://engineering.kablamo.com.au/",
    "title": [
        "Kablamo Engineering Blog"
    ],
    "h1": [
        "Lessons Learnt Building for the Atlassian Marketplace",
        "What I Wish I Knew About CSS When Starting Out As A Frontender",
        "How to model application flows in React with finite state machines and XState"
    ],
    "h2h3": [
        "Our Partners"
    ],
    "h4h5h6": null,
    "content": [
        "THE BLOG",
        "Insights from the Kablamo Team.",
        "28.7.2021 - By Ben Boyter"
    ]
}
</code></pre>
					</section>
					<section>
						<h2>Ranking</h2>
<pre><code style="font-size: 14px; line-height: 1em;">// defaults for BM25 which provide a good level of damping
k1 := 1.2
b := 0.75

for word, wordCount := range res.matchWords {
    freq := documentFrequencies[word]

    tf := float64(wordCount) / words
    idf := math.Log10(float64(corpusCount) / float64(freq))

    step1 := idf * tf * (k1 + 1)
    step2 := tf + k1*(1-b+(b*words/averageDocumentWords))

    weight += step1 / step2
}
</code></pre>
					</section>
					<section>
						<h2>Snippet Extraction</h2>
<blockquote>
features, noble mien, and the report which was in general circulation within five minutes after his entrance, of his having ten thousand a year. The gentlemen pronounced him to be a fine
</blockquote>

<blockquote>
it. Dear, dear Lizzy. A house in town! Every thing that is charming! Three daughters married! Ten thousand a year! Oh, Lord! What will become of me. I shall go distracted.”
</blockquote>

					</section>
				</section>


				<section>
					<section>
						<h2>Design</h2>
						<img src="./img/design.png" height="500px" style="border: none;" /><br>
						<aside class="notes">
API Gateway fronts a single controller lambda which calls multiple workers as configured in its environment variables. 
The workers contain the index, and when passed a query they search over their content finding matches, ranking them 
and then returning the top 20 results. The controller waits for all the results to return, joins them together, 
re-sorts and sends the top results back.

Usually having a lambda call another lambda is a major anti pattern, and it could probably be removed. 
I could just call the lambdas directly from the HTTP server at some point, although this way I get an API I can use
for other purposes, and it decouples the index from the web server which feels nice.
						</aside>
					</section>
					<section>
						<h2>Building</h2>
						<pre><code style="font-size: 14px; line-height: 1em;">
sb.WriteString(fmt.Sprintf(`var averageDocumentLength float64 = %d`, averageDocumentLength))

sb.WriteString(`var documentFrequencies = map[string]uint32{`)
for k, v := range newFreq {
    sb.WriteString(fmt.Sprintf("\"%s\": %d,", k, v))
}
sb.WriteString("}")

sb.WriteString("var bloomFilter = []uint64{")
for _, v := range bloom {
    sb.WriteString(fmt.Sprintf("%d,", v))
}
sb.WriteString("}")

_, _ = file.WriteString(fmt.Sprintf(`{Url:"%s",Title:"%s",Content:"%s",Score:%.4f},`,
			res.Url,
			res.Title,
			res.Content,
			res.Score))
</code></pre>
					</section>


					<section>
						<h2>Deployment</h2>
						<img src="./img/deployment.png" height="300px" style="border: none;" /><br>
<pre><code style="font-size: 14px; line-height: 1em;">
for i in {1..1000}
do
  cp ./content.go."$i" ./content.go
  GOOS=linux GOARCH=amd64 go build -ldflags="-s -w" -o main && 
  	zip main.zip main && 
	aws lambda update-function-code --function-name t"$i" --zip-file fileb://main.zip > /dev/null
done

aws lambda update-function-configuration --function-name aws-lambda-search-controller \
	--environment '{"Variables": {"WORKERS": "t0,t1,t2,t3,t4,t5,t6,t7,t8,t9,t10,t11,t12,t13,t14,t15,t16,t17,t18,t19,t20,t21,t22,t23,t24,t25,t26,t27,t28,t29,t30,t31,t32,t33,t34,t35,t36,t37,t38,t39,t40,t41,t42,t43,t44,t45,t46,t47,t48,t49,t50,t51,t52,t53,t54,t55,t56,t57,t58,t59,t60,t61,t62,t63,t64,t65,t66,t67,t68,t69,t70,t71,t72,t73,t74,t75,t76,t77,t78,t79,t80,t81,t82,t83,t84,t85,t86,t87,t88,t89,t90,t91,t92,t93,t94,t95,t96,t97,t98,t99,t100,t101,t102,t103,t104,t105,t106,t107,t108,t109,t110,t111,t112,t113,t114,t115,t116,t117,t118,t119,t120,t121,t122,t123,t124,t125,t126,t127,t128,t129,t130,t131,t132,t133,t134,t135,t136,t137,t138,t139,t140,t141,t142,t143,t144,t145,t146,t147,t148,t149,t150,t151,t152,t153,t154,t155,t156,t157,t158,t159,t160,t161,t162,t163,t164,t165,t166,t167,t168,t169,t170,t171,t172,t173,t174,t175,t176,t177,t178,t179,t180,t181,t182,t183,t184,t185,t186,t187,t188,t189,t190,t191,t192,t193,t194,t195,t196,t197,t198,t199,t200,t201,t202,t203,t204,t205,t206,t207,t208,t209,t210,t211,t212,t213,t214,t215,t216,t217,t218,t219,t220,t221,t222,t223,t224,t225,t226,t227,t228,t229,t230,t231,t232,t233,t234,t235,t236,t237,t238,t239,t240,t241,t242,t243,t244,t245,t246,t247,t248,t249,t250,t251,t252,t253,t254,t255,t256,t257,t258,t259,t260,t261,t262,t263,t264,t265,t266,t267,t268,t269,t270,t271,t272,t273,t274,t275,t276,t277,t278,t279,t280,t281,t282,t283,t284,t285,t286,t287,t288,t289,t290,t291,t292,t293,t294,t295,t296,t297,t298,t299,t300,t301,t302,t303,t304,t305,t306,t307,t308,t309,t310,t311,t312,t313,t314,t315,t316,t317,t318,t319,t320,t321,t322,t323,t324,t325,t326,t327,t328,t329,t330,t331,t332,t333,t334,t335,t336,t337,t338,t339,t340,t341,t342,t343,t344,t345,t346,t347,t348,t349,t350,t351,t352,t353,t354,t355,t356,t357,t358,t359,t360,t361,t362,t363,t364,t365,t366,t367,t368,t369,t370,t371,t372,t373,t374,t375,t376,t377,t378,t379,t380,t381,t382,t383,t384,t385,t386,t387,t388,t389,t390,t391,t392,t393,t394,t395,t396,t397,t398,t399,t400,t401,t402,t403,t404,t405,t406,t407,t408,t409,t410,t411,t412,t413,t414,t415,t416,t417,t418,t419,t420,t421,t422,t423,t424,t425,t426,t427,t428,t429,t430,t431,t432,t433,t434,t435,t436,t437,t438,t439,t440,t441,t442,t443,t444,t445,t446,t447,t448,t449,t450,t451,t452,t453,t454,t455,t456,t457,t458,t459,t460,t461,t462,t463,t464,t465,t466,t467,t468,t469,t470,t471,t472,t473,t474,t475,t476,t477,t478,t479,t480,t481,t482,t483,t484,t485,t486,t487,t488,t489,t490,t491,t492,t493,t494,t495,t496,t497,t498,t499,t500,t501,t502,t503,t504,t505,t506,t507,t508,t509,t510,t511,t512,t513,t514,t515,t516,t517,t518,t519,t520,t521,t522,t523,t524,t525,t526,t527,t528,t529,t530,t531,t532,t533,t534,t535,t536,t537,t538,t539,t540,t541,t542,t543,t544,t545,t546,t547,t548,t549,t550,t551,t552,t553,t554,t555,t556,t557,t558,t559,t560,t561,t562,t563,t564,t565,t566,t567,t568,t569,t570,t571,t572,t573,t574,t575,t576,t577,t578,t579,t580,t581,t582,t583,t584,t585,t586,t587,t588,t589,t590,t591,t592,t593,t594,t595,t596,t597,t598,t599,t600,t601,t602,t603,t604,t605,t606,t607,t608,t609,t610,t611,t612,t613,t614,t615,t616,t617,t618,t619,t620,t621,t622,t623,t624,t625,t626,t627,t628,t629,t630,t631,t632,t633,t634,t635,t636,t637,t638,t639,t640,t641,t642,t643,t644,t645,t646,t647,t648,t649,t650,t651,t652,t653,t654,t655,t656,t657,t658,t659,t660,t661,t662,t663,t664,t665,t666,t667,t668,t669,t670,t671,t672,t673,t674,t675,t676,t677,t678,t679,t680,t681,t682,t683,t684,t685,t686,t687,t688,t689,t690,t691,t692,t693,t694,t695,t696,t697,t698,t699,t700,t701,t702,t703,t704,t705,t706,t707,t708,t709,t710,t711,t712,t713,t714,t715,t716,t717,t718,t719,t720,t721,t722,t723,t724,t725,t726,t727,t728,t729,t730,t731,t732,t733,t734,t735,t736,t737,t738,t739,t740,t741,t742,t743,t744,t745,t746,t747,t748,t749,t750,t751,t752,t753,t754,t755,t756,t757,t758,t759,t760,t761,t762,t763,t764,t765,t766,t767,t768,t769,t770,t771,t772,t773,t774,t775,t776,t777,t778,t779,t780,t781,t782,t783,t784,t785,t786,t787,t788,t789,t790,t791,t792,t793,t794,t795,t796,t797,t798,t799,t800"}}'
</code></pre>

					</section>
				</section>




				<section>
					<h2>Conclusions</h2>
					<p>Go. I doubt I could have done it in another language.</p>
					<p>Bloomfilters + Trigrams works well. As far as I know this is globally unique. Nobody else I know uses this approach.</p>
					<p>A minimal version is available for you to play with <a href="https://github.com/boyter/indexer">https://github.com/boyter/indexer</a></p>
					<aside class="notes">
So conclusions.
I doubt I could have done this in another language. 
I like Go. I don't love it, it doesnt push the happy parts of my brain the way Python for example does.
But it just gets out of the way, and while I want to hurt myself every time I write another err nil check...
I realise I solved the problem... fairly quickly... the solution is solid and understandable and I just move on.
I guess that fits in with the boring that Go seems to aim for. Absolute success there.

The other is that Bloomfilters + Trigrams work really well. I actually asked chatgpt about this and it suggested
I shouldn't use them. 
Its also as far as I know unique approach with nobody having tried it before. Although nobody
in the search space really talk about what they are doing.

I have also uploaded a minimal version of everything for you to download and play around with.
					</aside>
				</section>

				<section>
					<p>Users probably don't care how searchcode's index works, but I know. Now you do too.</p>
					<img src="./img/gopher.png" height="500px" style="border: none;" /><br>
					<aside class="notes">


Of course... probably nobody using searchcode cares very much that its running on a unique bloom filter trigram
backed index with ideas borrowed from bing. 

But I know... and now you do too.

Thank you very much. Ill be at the Kablamo booth with the lovely stickers if you want to chat.
					</aside>
				</section>

			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
